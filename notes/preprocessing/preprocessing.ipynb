{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 511: Data acquisition and pre-processing <br>Chapter 4: Pre-processing considerations: foresight for downstream needs\n",
    "Preprocessing encapsulaates the set of operations that are needed in a project to simply prepare the data. Before you can start using the data that you have acquired to build what your project demands – be it visualizations, modeling, or otherwise – you need to get this data into a shape that will let you work with it. More often than not, acquired data will be in formats and structures that aren't useful for what you're trying to do. This is where preprocessing comes in. Since the data science workflow is often non-linear, you might find yourself coming back to preprocessing even after you thought you were done with it. Getting the data into right shape can take up more time, effort, and resources than you think.\n",
    "\n",
    "## 4.1 Converting Between File Types\n",
    "The two major structures of data that you will work with in Python are tables and dictionaries. Tabular data is usually stored in text files meant to be read line-by-line with each line containing values separated by a delimiter. Most commonly, when the delimiter is a comma, this means storing data in a CSV file. When you're working with dictionary-structured data, it's very convenient to store it in JSON. Often, the data you acquire will not be in the format you need it to be. In other cases, you might need the same data to be stored in multiple formats for different types of interaction. Here, we'll demonstrate some rudimentary conversion techniques between tabular and dictionary data.\n",
    "\n",
    "### 4.1.1 CSV to JSON\n",
    "First, we'll talk about taking data from a CSV file and transforming it to store it in JSON. We have an example CSV file, `colors.csv`, that contains some data about colors. When we load it and print out the contents, we can see that each row corresponds to a color and contains an ID, the name of the color, the hexadecimal code for the color, and the red, green, and blue values of the color for the RGB code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['air_force_blue_raf', 'Air Force Blue (Raf)', '#5d8aa8', '93', '138', '168'],\n",
      " ['air_force_blue_usaf', 'Air Force Blue (Usaf)', '#00308f', '0', '48', '143'],\n",
      " ['air_superiority_blue',\n",
      "  'Air Superiority Blue',\n",
      "  '#72a0c1',\n",
      "  '114',\n",
      "  '160',\n",
      "  '193'],\n",
      " ['alabama_crimson', 'Alabama Crimson', '#a32638', '163', '38', '56'],\n",
      " ['alice_blue', 'Alice Blue', '#f0f8ff', '240', '248', '255'],\n",
      " ['alizarin_crimson', 'Alizarin Crimson', '#e32636', '227', '38', '54'],\n",
      " ['alloy_orange', 'Alloy Orange', '#c46210', '196', '98', '16'],\n",
      " ['almond', 'Almond', '#efdecd', '239', '222', '205'],\n",
      " ['amaranth', 'Amaranth', '#e52b50', '229', '43', '80'],\n",
      " ['amber', 'Amber', '#ffbf00', '255', '191', '0']]\n"
     ]
    }
   ],
   "source": [
    "import csv, json\n",
    "from pprint import pprint\n",
    "\n",
    "reader = csv.reader(open(\"data/colors.csv\", \"r\")) \n",
    "color_lists = list(reader)\n",
    "\n",
    "pprint(color_lists[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can iterate over this list of colors and create dictionaries to store the values associated with each color. We can put these dictionaries in another list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'b': '168',\n",
      "  'g': '138',\n",
      "  'hex_value': '#5d8aa8',\n",
      "  'id': 'air_force_blue_raf',\n",
      "  'name': 'Air Force Blue (Raf)',\n",
      "  'r': '93'},\n",
      " {'b': '143',\n",
      "  'g': '48',\n",
      "  'hex_value': '#00308f',\n",
      "  'id': 'air_force_blue_usaf',\n",
      "  'name': 'Air Force Blue (Usaf)',\n",
      "  'r': '0'},\n",
      " {'b': '193',\n",
      "  'g': '160',\n",
      "  'hex_value': '#72a0c1',\n",
      "  'id': 'air_superiority_blue',\n",
      "  'name': 'Air Superiority Blue',\n",
      "  'r': '114'},\n",
      " {'b': '56',\n",
      "  'g': '38',\n",
      "  'hex_value': '#a32638',\n",
      "  'id': 'alabama_crimson',\n",
      "  'name': 'Alabama Crimson',\n",
      "  'r': '163'},\n",
      " {'b': '255',\n",
      "  'g': '248',\n",
      "  'hex_value': '#f0f8ff',\n",
      "  'id': 'alice_blue',\n",
      "  'name': 'Alice Blue',\n",
      "  'r': '240'},\n",
      " {'b': '54',\n",
      "  'g': '38',\n",
      "  'hex_value': '#e32636',\n",
      "  'id': 'alizarin_crimson',\n",
      "  'name': 'Alizarin Crimson',\n",
      "  'r': '227'},\n",
      " {'b': '16',\n",
      "  'g': '98',\n",
      "  'hex_value': '#c46210',\n",
      "  'id': 'alloy_orange',\n",
      "  'name': 'Alloy Orange',\n",
      "  'r': '196'},\n",
      " {'b': '205',\n",
      "  'g': '222',\n",
      "  'hex_value': '#efdecd',\n",
      "  'id': 'almond',\n",
      "  'name': 'Almond',\n",
      "  'r': '239'},\n",
      " {'b': '80',\n",
      "  'g': '43',\n",
      "  'hex_value': '#e52b50',\n",
      "  'id': 'amaranth',\n",
      "  'name': 'Amaranth',\n",
      "  'r': '229'},\n",
      " {'b': '0',\n",
      "  'g': '191',\n",
      "  'hex_value': '#ffbf00',\n",
      "  'id': 'amber',\n",
      "  'name': 'Amber',\n",
      "  'r': '255'}]\n"
     ]
    }
   ],
   "source": [
    "color_dicts = []\n",
    "\n",
    "for row in color_lists:\n",
    "    \n",
    "    color = {\n",
    "        \"id\" : row[0],\n",
    "        \"name\" : row[1],\n",
    "        \"hex_value\" : row[2],\n",
    "        \"r\" : row[3],\n",
    "        \"g\" : row[4],\n",
    "        \"b\" : row[5]\n",
    "    }\n",
    "    \n",
    "    color_dicts.append(color)\n",
    "    \n",
    "pprint(color_dicts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, it is easy to store this list of dictionaries as a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(color_dicts, open(\"data/color-dicts.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1.1 Exercise: CSV to JSON conversion\n",
    "Read the `cities.csv` file and look at its contents. It should have a header (the first line of the file) that tells you which fields contain what data. Next, take the data for  only the cities which have their population listed and store this in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 JSON to CSV\n",
    "Now we'll explore converting JSON data to tabular formats. In `nobel-laureates.json`, we have data about more than 900 recipients of the Nobel Prize. Let's load this in and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'born': '1845-03-27',\n",
      "  'bornCity': 'Lennep (now Remscheid)',\n",
      "  'bornCountry': 'Prussia (now Germany)',\n",
      "  'bornCountryCode': 'DE',\n",
      "  'died': '1923-02-10',\n",
      "  'diedCity': 'Munich',\n",
      "  'diedCountry': 'Germany',\n",
      "  'diedCountryCode': 'DE',\n",
      "  'firstname': 'Wilhelm Conrad',\n",
      "  'gender': 'male',\n",
      "  'id': '1',\n",
      "  'prizes': [{'affiliations': [{'city': 'Munich',\n",
      "                                'country': 'Germany',\n",
      "                                'name': 'Munich University'}],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"in recognition of the extraordinary services he '\n",
      "                            'has rendered by the discovery of the remarkable '\n",
      "                            'rays subsequently named after him\"',\n",
      "              'share': '1',\n",
      "              'year': '1901'}],\n",
      "  'surname': 'Röntgen'},\n",
      " {'born': '1853-07-18',\n",
      "  'bornCity': 'Arnhem',\n",
      "  'bornCountry': 'the Netherlands',\n",
      "  'bornCountryCode': 'NL',\n",
      "  'died': '1928-02-04',\n",
      "  'diedCountry': 'the Netherlands',\n",
      "  'diedCountryCode': 'NL',\n",
      "  'firstname': 'Hendrik Antoon',\n",
      "  'gender': 'male',\n",
      "  'id': '2',\n",
      "  'prizes': [{'affiliations': [{'city': 'Leiden',\n",
      "                                'country': 'the Netherlands',\n",
      "                                'name': 'Leiden University'}],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"in recognition of the extraordinary service they '\n",
      "                            'rendered by their researches into the influence '\n",
      "                            'of magnetism upon radiation phenomena\"',\n",
      "              'share': '2',\n",
      "              'year': '1902'}],\n",
      "  'surname': 'Lorentz'},\n",
      " {'born': '1865-05-25',\n",
      "  'bornCity': 'Zonnemaire',\n",
      "  'bornCountry': 'the Netherlands',\n",
      "  'bornCountryCode': 'NL',\n",
      "  'died': '1943-10-09',\n",
      "  'diedCity': 'Amsterdam',\n",
      "  'diedCountry': 'the Netherlands',\n",
      "  'diedCountryCode': 'NL',\n",
      "  'firstname': 'Pieter',\n",
      "  'gender': 'male',\n",
      "  'id': '3',\n",
      "  'prizes': [{'affiliations': [{'city': 'Amsterdam',\n",
      "                                'country': 'the Netherlands',\n",
      "                                'name': 'Amsterdam University'}],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"in recognition of the extraordinary service they '\n",
      "                            'rendered by their researches into the influence '\n",
      "                            'of magnetism upon radiation phenomena\"',\n",
      "              'share': '2',\n",
      "              'year': '1902'}],\n",
      "  'surname': 'Zeeman'},\n",
      " {'born': '1852-12-15',\n",
      "  'bornCity': 'Paris',\n",
      "  'bornCountry': 'France',\n",
      "  'bornCountryCode': 'FR',\n",
      "  'died': '1908-08-25',\n",
      "  'diedCountry': 'France',\n",
      "  'diedCountryCode': 'FR',\n",
      "  'firstname': 'Antoine Henri',\n",
      "  'gender': 'male',\n",
      "  'id': '4',\n",
      "  'prizes': [{'affiliations': [{'city': 'Paris',\n",
      "                                'country': 'France',\n",
      "                                'name': 'École Polytechnique'}],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"in recognition of the extraordinary services he '\n",
      "                            'has rendered by his discovery of spontaneous '\n",
      "                            'radioactivity\"',\n",
      "              'share': '2',\n",
      "              'year': '1903'}],\n",
      "  'surname': 'Becquerel'},\n",
      " {'born': '1859-05-15',\n",
      "  'bornCity': 'Paris',\n",
      "  'bornCountry': 'France',\n",
      "  'bornCountryCode': 'FR',\n",
      "  'died': '1906-04-19',\n",
      "  'diedCity': 'Paris',\n",
      "  'diedCountry': 'France',\n",
      "  'diedCountryCode': 'FR',\n",
      "  'firstname': 'Pierre',\n",
      "  'gender': 'male',\n",
      "  'id': '5',\n",
      "  'prizes': [{'affiliations': [{'city': 'Paris',\n",
      "                                'country': 'France',\n",
      "                                'name': 'École municipale de physique et de '\n",
      "                                        'chimie industrielles (Municipal '\n",
      "                                        'School of Industrial Physics and '\n",
      "                                        'Chemistry)'}],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"in recognition of the extraordinary services '\n",
      "                            'they have rendered by their joint researches on '\n",
      "                            'the radiation phenomena discovered by Professor '\n",
      "                            'Henri Becquerel\"',\n",
      "              'share': '4',\n",
      "              'year': '1903'}],\n",
      "  'surname': 'Curie'},\n",
      " {'born': '1867-11-07',\n",
      "  'bornCity': 'Warsaw',\n",
      "  'bornCountry': 'Russian Empire (now Poland)',\n",
      "  'bornCountryCode': 'PL',\n",
      "  'died': '1934-07-04',\n",
      "  'diedCity': 'Sallanches',\n",
      "  'diedCountry': 'France',\n",
      "  'diedCountryCode': 'FR',\n",
      "  'firstname': 'Marie',\n",
      "  'gender': 'female',\n",
      "  'id': '6',\n",
      "  'prizes': [{'affiliations': [[]],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"in recognition of the extraordinary services '\n",
      "                            'they have rendered by their joint researches on '\n",
      "                            'the radiation phenomena discovered by Professor '\n",
      "                            'Henri Becquerel\"',\n",
      "              'share': '4',\n",
      "              'year': '1903'},\n",
      "             {'affiliations': [{'city': 'Paris',\n",
      "                                'country': 'France',\n",
      "                                'name': 'Sorbonne University'}],\n",
      "              'category': 'chemistry',\n",
      "              'motivation': '\"in recognition of her services to the '\n",
      "                            'advancement of chemistry by the discovery of the '\n",
      "                            'elements radium and polonium, by the isolation of '\n",
      "                            'radium and the study of the nature and compounds '\n",
      "                            'of this remarkable element\"',\n",
      "              'share': '1',\n",
      "              'year': '1911'}],\n",
      "  'surname': 'Curie, née Sklodowska'},\n",
      " {'born': '1842-11-12',\n",
      "  'bornCity': 'Langford Grove, Maldon, Essex',\n",
      "  'bornCountry': 'United Kingdom',\n",
      "  'bornCountryCode': 'GB',\n",
      "  'died': '1919-06-30',\n",
      "  'diedCountry': 'United Kingdom',\n",
      "  'diedCountryCode': 'GB',\n",
      "  'firstname': 'Lord Rayleigh',\n",
      "  'gender': 'male',\n",
      "  'id': '8',\n",
      "  'prizes': [{'affiliations': [{'city': 'London',\n",
      "                                'country': 'United Kingdom',\n",
      "                                'name': 'Royal Institution of Great Britain'}],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"for his investigations of the densities of the '\n",
      "                            'most important gases and for his discovery of '\n",
      "                            'argon in connection with these studies\"',\n",
      "              'share': '1',\n",
      "              'year': '1904'}],\n",
      "  'surname': '(John William Strutt)'},\n",
      " {'born': '1862-06-07',\n",
      "  'bornCity': 'Pressburg (now Bratislava)',\n",
      "  'bornCountry': 'Hungary (now Slovakia)',\n",
      "  'bornCountryCode': 'SK',\n",
      "  'died': '1947-05-20',\n",
      "  'diedCity': 'Messelhausen',\n",
      "  'diedCountry': 'Germany',\n",
      "  'diedCountryCode': 'DE',\n",
      "  'firstname': 'Philipp Eduard Anton',\n",
      "  'gender': 'male',\n",
      "  'id': '9',\n",
      "  'prizes': [{'affiliations': [{'city': 'Kiel',\n",
      "                                'country': 'Germany',\n",
      "                                'name': 'Kiel University'}],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"for his work on cathode rays\"',\n",
      "              'share': '1',\n",
      "              'year': '1905'}],\n",
      "  'surname': 'von Lenard'},\n",
      " {'born': '1856-12-18',\n",
      "  'bornCity': 'Cheetham Hill, near Manchester',\n",
      "  'bornCountry': 'United Kingdom',\n",
      "  'bornCountryCode': 'GB',\n",
      "  'died': '1940-08-30',\n",
      "  'diedCity': 'Cambridge',\n",
      "  'diedCountry': 'United Kingdom',\n",
      "  'diedCountryCode': 'GB',\n",
      "  'firstname': 'Joseph John',\n",
      "  'gender': 'male',\n",
      "  'id': '10',\n",
      "  'prizes': [{'affiliations': [{'city': 'Cambridge',\n",
      "                                'country': 'United Kingdom',\n",
      "                                'name': 'University of Cambridge'}],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"in recognition of the great merits of his '\n",
      "                            'theoretical and experimental investigations on '\n",
      "                            'the conduction of electricity by gases\"',\n",
      "              'share': '1',\n",
      "              'year': '1906'}],\n",
      "  'surname': 'Thomson'},\n",
      " {'born': '1852-12-19',\n",
      "  'bornCity': 'Strelno (now Strzelno)',\n",
      "  'bornCountry': 'Prussia (now Poland)',\n",
      "  'bornCountryCode': 'PL',\n",
      "  'died': '1931-05-09',\n",
      "  'diedCity': 'Pasadena, CA',\n",
      "  'diedCountry': 'USA',\n",
      "  'diedCountryCode': 'US',\n",
      "  'firstname': 'Albert Abraham',\n",
      "  'gender': 'male',\n",
      "  'id': '11',\n",
      "  'prizes': [{'affiliations': [{'city': 'Chicago, IL',\n",
      "                                'country': 'USA',\n",
      "                                'name': 'University of Chicago'}],\n",
      "              'category': 'physics',\n",
      "              'motivation': '\"for his optical precision instruments and the '\n",
      "                            'spectroscopic and metrological investigations '\n",
      "                            'carried out with their aid\"',\n",
      "              'share': '1',\n",
      "              'year': '1907'}],\n",
      "  'surname': 'Michelson'}]\n"
     ]
    }
   ],
   "source": [
    "nobel_laureates = json.load(open(\"data/nobel-laureates.json\", \"r\"))\n",
    "\n",
    "pprint(nobel_laureates[\"laureates\"][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to take some of this data and put it in a table. We're interested in the first name, surname, and the year of receiving the prize for each laureate. However, if we inspect the data we can see that there are, inexplicably, some entries that are empty of any information, such as they don't even list a name. So, we need to build this condition into our code: a valid laureate must have a first name. Sometimes, the Prize is given to organizations, and organizations have no surnames. In these cases, we'll want to leave the surname empty. Some laureates (for example Marie Curie) may have won multiple Nobel Prizes, in different years. For these cases, we'll want to build the `year` string as containing the different years with a space character in between.  Keeping all these considerations in mind, we can convert the data into a tabular format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "laureates_table = []\n",
    "\n",
    "for n, laureate in enumerate(nobel_laureates[\"laureates\"]):\n",
    "    \n",
    "    if \"firstname\" in laureate.keys():\n",
    "        \n",
    "        if \"surname\" in laureate.keys():\n",
    "            surname = laureate[\"surname\"]\n",
    "        else:\n",
    "            surname = \"\"\n",
    "\n",
    "        years = []\n",
    "\n",
    "        for prize in laureate[\"prizes\"]:\n",
    "            years.append(prize[\"year\"])\n",
    "\n",
    "        if len(years) == 1:\n",
    "            years = years[0]\n",
    "        else:\n",
    "            years = \" \".join(years)\n",
    "\n",
    "\n",
    "\n",
    "        firstname = laureate[\"firstname\"]\n",
    "\n",
    "        row = [surname, firstname, years]\n",
    "\n",
    "        laureates_table.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some laureates' names might actually contain commas. So in this case, a comma would not be a good delimiter for the data. We can instead use a tab character `\\t` as the delimiter and save this data to a \"tab-separated values\" or \".tsv\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/nobel-laureates-info.tsv\", \"w\") as f:\n",
    "    for laureate in laureates_table:\n",
    "        f.write(\"\\t\".join(laureate) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2.1 Exercise: JSON to CSV conversion\n",
    "Load the data in the `american-movies.json` file. We only want the movies that were made from 1990 to 1999 (it was a truly glorious decade for American cinema). Your task is to take the title and year of making for these movies and put these in a tab-separated values file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2.2 Can we always go both ways?\n",
    "One thing to keep in mind is that CSV-like formats can't really handle nesting. Because of this, JSON files that contain nested structures will not translate into tabular formats. The utility of tabular formats is that they are simply text files that require minimal additional processing to read and write, so very often the same data will be faster to access and store in tabular formats than in JSON. File size will be generally smaller, too. However, more complexly structured data isn't compatible with tables.\n",
    "\n",
    "In general, the structure of a JSON file is that of an _associative array_, meaning data is organized in key-value pairs. Associative arrays are great for data that isn't sequentially organized. Low-level, in-memory implementations of associative arrays make searching for values fast and reliable. Also, as we've mentioned, associative arrays make storing more complex structures possible. Tabular formats are analogous to _ordered arrays_ (think lists in Python). In an ordered array, the data is sequentially arranged. This means, rather than  access the data using keys, they are accessed using indices. However, since there are no keys, performing search operations are computationally costly. Ordered arrays are good for storing data that doesn't need to be searched very often and needs to be accessed sequentially. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2.3 Is the JSON file scalable\n",
    "You might've noticed that the top-level structure of the `'nobel-laureates.json'` file to be a dictionary, even though it only had one key: `'laureates'`, underneith of which is a list of all of the data _records_. In this example, a completely ordered `.csv` structure was not practical, since the value of each record's `'prizes'` field is a variable-length list (\n",
    "this is why the prize years were joined together as a space-delimited string in the conversion\n",
    "). Despite a lack of flexibility, a major benefit of tabular serializations is their convenience for line-by-line file reading, specifically when each row is an independent record. Reading a very large file line-by-line prevents your computer's memory from being overloaded and is a basic requirement of hadoop etc. and disk-based implementations of the map-reduce programming pattern. \n",
    "\n",
    "#### 4.1.2.4 Exercise: Making JSON file reading scalable\n",
    "Create a specialized JSON serialization of the data in `'nobel-laureates.json'`. Specifically, create a file called `'data/nobel-laureates-lines.json'` that has each lauriate's record serialized seprately as a json object, with newlines `'\\n'` in between, as delimiters. As a follow up, combine the line-by-line file reading syntax introduced in Section 1.4.1.5 in conjunction with the `json.dumps()` string serialization function in Section 1.4.2.2 to _read only the first ten lines_. As you read these lines, load each from json and print the laureate's list of prizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 The 4 V's of Big Data\n",
    "The 4 V's of Big Data refer to 4 important considerations to keep in mind when working with large data sets, and all of these are important when dealing with the preprocessing phase of a project.\n",
    "\n",
    "- __Volume__: This refers to the overall size of a data set, whether in terms of KB, MB, GB, or TB, etc.; its number of records/rows, or represented individuals. In the case of volume, big data refers to the capacity for _quantity_ to create processing challenges, which if overcome might lead to game-changing opportunities.\n",
    "- __Velocity__: This refers to the _rate_ at which data are being produced. Sometimes, prolonged collection of high-velocity data will result in a data set that is big for reasons of volume. However, velocity can create big data challenges and opportunities independent of volume. Rapid rates of data generation can create opportunities for real-time analysis, where we know whats going on right now in the world.\n",
    "- __Variety__: The whole may be greater than the sum of its parts. Having more dimensions through which to view an individual or data object can really enrich the outcome for an analysis. However, variety van be challengingm, too: working with high-variety data with many linked dimensions and/or mediums might make it unclear where an analysis should start, or how those dimensions fit together into a bigger picture. A project working with high-variety data will probably spend a lot of time in the weeds with exploratory data analysis just trying to figure out which data are actually useful.\n",
    "- __Veracity__: This refers to the _reliability_ of data. While it's hard to imagine when data veracity can be a helpful feature (unlike volume, velocity, or variety), it's pretty easy to encapsulate veracity as a big data problem: can you trust the data that you're working with? Here's perhaps one way that veracity might be beneficial: in unstructured data types, like text, a phenomenon is recorded as it operates, and without the imposition of structure by a data collector. This allows for the full _variety_ of behaviors that might occur to occur, at the expense of a reliability mess.\n",
    "\n",
    "In particular, annotating data reliably is a major issue. Different types of annotation include:\n",
    "\n",
    "- __Sensed labels__: Some devices can provide precise data labels, like geographic annotations via gps and latitude/longitude pairs. \n",
    "- __Unsupervised cues__: Some researchers will use observable features as proxies for a desired annotation, e.g., like using keywords in text to label document topics.\n",
    "- __User-provided tags__: Some data-production platforms regularize symbols for users to apply to their content. Just like webpage meta tags embed annotations into webpages for search engines, hashtags are data annoations on Twitter that cue topics and link content. Here, it's up to the user to apply the right annotations to their data.\n",
    "- __Third-party reviews__: This is probably the most common annotation strategy for researchers, and generally occurs after data are generated. Most commonly, a high-quality and representative sample (see __Sec. 1.1.4__) of data are passed out to individuals who are trained to annotate or know about the phenomenon of interest. This might mean doing it yourself, asking/paying graduate students to complete an annotation task, or creating a large survey and distributing it electronically.\n",
    "    - __Amazon's Mechanical Turk__: [This](https://www.mturk.com/) is an online distributed survey service provided by Amazon that connects survey writers (for a fee) to a very large network of survey takers. The survey takers generally come from across the globe and sometimes even live off of these Mechanical Turn \"hits\" (surveys) as a form of primary income. For researchers, this has become a quick and (relatively) easy way to annotate large quantities of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Randomness\n",
    "\n",
    "An important module we often need to use is the `random` module. In particular, it is very useful for taking random samples from large volumes of data. For example, we could use the `random.sample()` function to take a sample from `color_lists`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['raw_umber', 'Raw Umber', '#826644', '130', '102', '68'],\n",
      " ['pale_violet_red', 'Pale Violet-Red', '#db7093', '219', '112', '147'],\n",
      " ['orange_web_color', 'Orange (Web Color)', '#ffa500', '255', '165', '0'],\n",
      " ['cherry', 'Cherry', '#de3163', '222', '49', '99'],\n",
      " ['light_crimson', 'Light Crimson', '#f56991', '245', '105', '145']]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "color_sample = random.sample(color_lists, 5)\n",
    "\n",
    "pprint(color_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['red_pigment', 'Red (Pigment)', '#ed1c24', '237', '28', '36'],\n",
      " ['medium_champagne', 'Medium Champagne', '#f3e5ab', '243', '229', '171'],\n",
      " ['light_slate_gray', 'Light Slate Gray', '#789', '119', '136', '153'],\n",
      " ['midnight_green_eagle_green',\n",
      "  'Midnight Green (Eagle Green)',\n",
      "  '#004953',\n",
      "  '0',\n",
      "  '73',\n",
      "  '83'],\n",
      " ['coral_red', 'Coral Red', '#ff4040', '255', '64', '64']]\n"
     ]
    }
   ],
   "source": [
    "color_sample = random.sample(color_lists, 5)\n",
    "\n",
    "pprint(color_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['violet', 'Violet', '#8f00ff', '143', '0', '255'],\n",
      " ['saddle_brown', 'Saddle Brown', '#8b4513', '139', '69', '19'],\n",
      " ['vivid_auburn', 'Vivid Auburn', '#922724', '146', '39', '36'],\n",
      " ['dark_sienna', 'Dark Sienna', '#3c1414', '60', '20', '20'],\n",
      " ['lavender_purple', 'Lavender Purple', '#967bb6', '150', '123', '182']]\n"
     ]
    }
   ],
   "source": [
    "color_sample = random.sample(color_lists, 5)\n",
    "\n",
    "pprint(color_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to perform _reproducible_ random operations by setting the value of `random.seed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['raspberry_glace', 'Raspberry Glace', '#915f6d', '145', '95', '109'],\n",
      " ['cadmium_yellow', 'Cadmium Yellow', '#fff600', '255', '246', '0'],\n",
      " ['arsenic', 'Arsenic', '#3b444b', '59', '68', '75'],\n",
      " ['straw', 'Straw', '#e4d96f', '228', '217', '111'],\n",
      " ['electric_crimson', 'Electric Crimson', '#ff003f', '255', '0', '63']]\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "color_sample = random.sample(color_lists, 5)\n",
    "\n",
    "pprint(color_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['raspberry_glace', 'Raspberry Glace', '#915f6d', '145', '95', '109'],\n",
      " ['cadmium_yellow', 'Cadmium Yellow', '#fff600', '255', '246', '0'],\n",
      " ['arsenic', 'Arsenic', '#3b444b', '59', '68', '75'],\n",
      " ['straw', 'Straw', '#e4d96f', '228', '217', '111'],\n",
      " ['electric_crimson', 'Electric Crimson', '#ff003f', '255', '0', '63']]\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "color_sample = random.sample(color_lists, 5)\n",
    "\n",
    "pprint(color_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4.4 Pre-processing text data\n",
    "Text data is abundant, and very often you will find yourself working with it. In this section, we'll discuss some powerful tools and techniques for text data manipulation that can come in very handy in preprocessing.\n",
    "\n",
    "### 4.4.1 Regular expressions\n",
    "\n",
    "Regular expressions, or regex, are \"sequences of characters that define a search pattern\", according to Wikipedia. These patterns can be used to search for, find, replace, and do a great deal more with strings.\n",
    "\n",
    "Regular expression patterns are constructed with both ordinary and special characters. The simplest regular expressions are simply ordinary characters like \"A\", or \"5\", or \"status\". These patterns only match themselves, allowing you to search for exact patterns of characters. Some characters are \"special\" for regex, like \"|\" or \"\\[\" or \"^\". These characters can be used to construct regex that is more powerful than straightforward matching.\n",
    "\n",
    "#### 4.4.1.1 Basics\n",
    "\n",
    "Python's included `re` module can be used to construct and use regular expressions. It comes with many useful functions. The most basic of match object if the pattern matched the string and a `None` value if it didn't. This means `re.search()` outputs can be used with conditional statements (like `if` statements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 8), match='fish'>\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "silly_string = \"one fish two fish red fish blue fish\"\n",
    "\n",
    "print(re.search(\"fish\", silly_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.search(\"salmon\", silly_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fish were found.\n"
     ]
    }
   ],
   "source": [
    "if re.search(\"fish\", silly_string):\n",
    "    print(\"Fish were found.\")\n",
    "else:\n",
    "    print(\"There were no fish.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful function is `re.sub()`, which takes two patterns and a string as input and replaces the first pattern with the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one cat two cat red cat blue cat\n"
     ]
    }
   ],
   "source": [
    "silly_cats = re.sub(\"fish\", \"cat\", silly_string)\n",
    "\n",
    "print(silly_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.findall()` will return all matches of a pattern in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'cat', 'cat', 'cat']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"cat\", silly_cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we have some text that we suspect contains Philadelphia area ZIP codes, we could use character classes to extract these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19104', '19102', '19107']\n"
     ]
    }
   ],
   "source": [
    "text = \"Drexel's University City campus falls in 19104, while the Collge of Nursing is in 19102 and the Philadelphia City Hall is in 19107.\"\n",
    "\n",
    "zipcodes = re.findall(\"191[0-5][0-9]\", text) # we know philly zipcodes go from 19102 to 19154\n",
    "print(zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1.2 A few useful character classes and other means for flexibility\n",
    "\n",
    "- `.` __(wild card)__ In the default mode, this matches any character except a newline.\n",
    "- `[...]` __(character class)__ Used to indicate flexible matching across a specificed set of characters.\n",
    "- `[^...]` __(complimentary character class)__ Used to indicate flexible matching across _everything but_ a specificed set of characters.\n",
    "- `[a-z]` __(lowercase range)__ Used to indicate flexible matching across lowercase letter ranges\n",
    "- `[A-Z]` __(uppercase range)__ Used to indicate flexible matching across uppercase letter ranges\n",
    "- `[0-9]` __(numeric range)__ Used to indicate flexible matching across numeric ranges\n",
    "- '|' __(or)__ Creates a regular expression that will match either A or B. \n",
    "\n",
    "Like the `string.split()` method, `re` also has a `re.split()` method that can be used with regex patterns. We could combine this with a character class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Oftentimes', ' different punctuation characters are used', ' these indicate different types of stops', '']\n"
     ]
    }
   ],
   "source": [
    "not_a_silly_string = \"Oftentimes, different punctuation characters are used; these indicate different types of stops.\"\n",
    "\n",
    "## split a string by several types of punctuation\n",
    "clauses = re.split(\"[,;.]\", not_a_silly_string)\n",
    "print(clauses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have some text that we suspect contains Philadelphia area ZIP codes, we could use character classes to extract these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19104', '19102', '19107']\n"
     ]
    }
   ],
   "source": [
    "text = \"Drexel's University City campus falls in 19104, while the Collge of Nursing is in 19102 and the Philadelphia City Hall is in 19107.\"\n",
    "\n",
    "zipcodes = re.findall(\"191[0-5][0-9]\", text) # we know philly zipcodes go from 19102 to 19154\n",
    "print(zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1.3 Exercise: Regex phone numbers\n",
    "Read the file `phone-numbers.txt`. It contains a phone number in each line. \\[Hint: use something like `lines = open(\"file.txt\", \"r\").readlines()`\\] Store only the phone numbers with the area code \"215\" in a list and print it out. Use regex-based pattern matching, not any other methods which occur to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1.4 Grouping, numbered groups and extensions\n",
    "Grouping is a great way to modify and extend strings, without simply replacing them. With grouping, you can use the matched content in a substitute string. It's great for re-formatting text. Groups can also serve extended functions if they are initiated by an unescaped question mark.\n",
    "- `(...)` __(group)__ Matches whatever regular expression is inside the parentheses, and indicates the start and end of a group; the contents of a group can be retrieved after a match has been performed, and can be matched later in the string with the `\\1`, `\\2`, etc., special sequences, described below.\n",
    "- `\\1`, `\\2`, etc. __(captured groups)__ Matched groups are captured and held in order: low to high from left to right, and in the case of nested groups, from outside to inside.\n",
    "- `(?...)` __(non-matching group)__ Matches `...` as in the parentheses, but does not capture it in a group. This becomes especially important when applying multipliers.\n",
    "- `(?=...)` __(lookahead)__ Matches if `...` matches next, but doesn’t consume any of the string.\n",
    "- `(?!...)` __(negative look ahead)__ Matches if `...` doesn’t match next.\n",
    "- `(?<=...)` __(positive look behind)__ Matches if the current position in the string is preceded by a match for `...` that ends at the current position. \n",
    "- `(?<!...)` __(negative look behind)__ Matches if the current position in the string is not preceded by a match for `...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apparently, 1-800-867-5307 is Jenny's phone number, but I'm not sure what her area code is.\n"
     ]
    }
   ],
   "source": [
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "\n",
    "## let's capture Jenny's phone number and insert the area code\n",
    "modified_tommy_two_tone = re.sub(r\"([0-9][0-9][0-9]-[0-9][0-9][0-9][0-9])\",r\"1-800-\\1\", tommy_two_tone)\n",
    "\n",
    "print(modified_tommy_two_tone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1.5 Multipliers (quantifiers)\n",
    "It was a little bit of overkill to use the numeric character class so many times in a row in the last expression. This is an example of where multiplies can come in really handy.\n",
    "- `*` __(zero or more)__ Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible. \n",
    "- `+` __(one or more)__ Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’.\n",
    "- `?` __(zero or one)__ Causes the resulting RE to match 0 or 1 repetitions of the preceding RE.\n",
    "- `{m}` __(exactly m times)__ Specifies that exactly m copies of the previous RE should be matched.\n",
    "- `{m,n}` __(m throug n times)__ Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as many repetitions as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['867-5307']\n"
     ]
    }
   ],
   "source": [
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "\n",
    "## let's get all of the phone numbers in a string\n",
    "numbers = re.findall(\"[0-9]{3}-[0-9]{4}\", tommy_two_tone)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', '867-5307'), ('Jenny', '')]\n"
     ]
    }
   ],
   "source": [
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "\n",
    "## capture the word that appears before a lookahead: \"'s phone number\" \n",
    "## by matching one or more non-space characters before \n",
    "## along with the number itself with flexible \"|\" matching\n",
    "whos_number = re.findall(\"([^ ]+)(?='s phone number)|([0-9]{3}-[0-9]{4})\", tommy_two_tone)\n",
    "\n",
    "print(whos_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['867-5307']\n",
      "['215-895-2185']\n"
     ]
    }
   ],
   "source": [
    "## We can even get a bit more flexible with our area-code handling!\n",
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "my_contact_information = \"If you need my office line, it's 215-895-2185\"\n",
    "\n",
    "## By grouping and using a `{1,2}` flexible match, we can get full and partial numbers\n",
    "## Note: we have to use a non-capturing group in order to make sure we get the full expression\n",
    "## without capturing the first three digits, only.\n",
    "numbers =  re.findall(\"(?:[0-9]{3}-){1,2}[0-9]{4}\", tommy_two_tone)\n",
    "print(numbers)\n",
    "numbers =  re.findall(\"(?:[0-9]{3}-){1,2}[0-9]{4}\", my_contact_information)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1.6 Escapes and special sequences\n",
    "As it turns out, some character classes are so common that they have their own special-characters. So, our phone-number example could be even more concise with the `\\d` special character.\n",
    "- `\\` __(escape)__ Either escapes special characters (permitting you to match characters like `*`, `?`, and so forth), or signals a special sequence.\n",
    "- `\\d` __(digits)__ Matches any Unicode decimal digit. This includes `[0-9]`, and also many other digit characters.\n",
    "- `\\D` __(digits)__ Matches any Unicode non-digit.\n",
    "- ` \\s` __(whitespace)__ Matches Unicode whitespace characters, including `[\\t\\n\\r]` and space.\n",
    "- `\\w` __(word characters)__ Matches Unicode word characters; this includes most characters that can be part of a word in any language, as well as numbers and the underscore.\n",
    "- `\\W` __(non-word characters)__ Matches Unicode non-word characters;\n",
    "- `\\t` __(tab)__ Matches a tab character.\n",
    "- `\\n` __(newline)__ matches a newline character.\n",
    "- `\\r` __(carriage return)__ matches a carriage return character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['867-5307']\n"
     ]
    }
   ],
   "source": [
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "\n",
    "## let's get all of the phone numbers in a string\n",
    "numbers = re.findall(\"(?:\\d{3}){1,2}-\\d{4}\", tommy_two_tone)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1.7 Anchors\n",
    "Anchors allow you to make the positions of matches absolute in the overally position in a string. These become especially handy if you are pre-processing semi-structured text, like a screenplay, stenographer's court record, or the index of a book.\n",
    "- `^` __(start anchor)__ Matches the start of the string.\n",
    "- `$` __(end anchor)__ Matches the end of the string or just before the newline at the end of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Witch: When shall we three meet again? In thunder, lightning, or in rain?\n",
      "Second Witch: When the hurlyburly's done, when the battle's lost and won.\n",
      "\n",
      "['First Witch', 'Second Witch']\n",
      "\n",
      "['When shall we three meet again? In thunder, lightning, or in rain?', \"When the hurlyburly's done, when the battle's lost and won.\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## an example of some sem-structured text\n",
    "macbeth = \"First Witch: When shall we three meet again? In thunder, lightning, or in rain?\\nSecond Witch: When the hurlyburly's done, when the battle's lost and won.\"\n",
    "print(macbeth)\n",
    "print(\"\")\n",
    "\n",
    "## make some empty lists for our data\n",
    "speakers = []\n",
    "speeches = []\n",
    "\n",
    "## split the document into the lines of the play\n",
    "lines = macbeth.split(\"\\n\")\n",
    "\n",
    "## loop over the lines\n",
    "for line in lines:\n",
    "    \n",
    "    ## retrieve the matched groups\n",
    "    ## Note: if we simply split by a colon \n",
    "    ## we might mess up what people are saying in the text!\n",
    "    ## Also note: the super greedy \".*?\" matching ANYTHING, zero or more times!\n",
    "    ## This comes in very handy when you want loosely anything\n",
    "    ## that happens to be surrounded by some specified structure\n",
    "    speaker, speech = re.search(\"^(.*?): (.*?)$\", line).groups()\n",
    "\n",
    "    ## Grow the lists\n",
    "    speakers.append(speaker)\n",
    "    speeches.append(speech)\n",
    "\n",
    "print(speakers)\n",
    "print(\"\")\n",
    "print(speeches)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1.8 Exercise: Names of the gods\n",
    "In the cell below is some text. It's an extract from [A Clash of Kings](https://www.goodreads.com/book/show/10572.A_Clash_of_Kings), specifically, about a character's prayer to some fictional gods. Use regex to extract the names of these gods. Your output should be a list that looks something like `[\"the Father\", \"the Mother\", \"the Warrior\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Lost and weary, Catelyn Stark gave herself over to her gods. She knelt before the Smith, who fixed things that were broken, and asked that he give her sweet Bran his protection. She went to the Maid and beseeched her to lend her courage to Arya and Sansa, to guard them in their innocence. To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, and she asked the Warrior to keep Robb strong and shield him in his battles. Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \"Guide me, wise lady,\" she prayed. \"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"'\n",
    "\n",
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Tokenization\n",
    "Tokenization is the process of breaking up text into smaller units. Usually, this means breaking a string up into words. The simplest possible tokenization would be to use the `string.split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this is, punctuation has been captured as part of some words. For a more advanced tokenizer, we'll use one of the most well-known Python modules for natural language processing, the Natural Language Toolkit (`nltk`). (Install it with `pip3 install nltk`, then import it with `import nltk` and run `nltk.download()`, which will open up a graphical window and allow you to download the data NLTK needs to perform many tasks.) ([Docs](https://www.nltk.org/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A newer set of tools can be found in the `spacy` module (`pip3 install spacy`). ([Docs](https://spacy.io/usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', '\\n', 'in', 'New', 'York', '.', ' ', 'Please', 'buy', 'me', '\\n', 'two', 'of', 'them', '.', '\\n\\n', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# spacy creates \"token\" objects which have quite a few properties. Check the documentation out if you're interested in learning more.\n",
    "\n",
    "words = []\n",
    "\n",
    "for token in doc:\n",
    "    words.append(token.text)\n",
    "    \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem – as you may have noticed – is that there are variations between tokenizers that can result in different outcomes for you further down the line. Choice of tokenizer can make or break a particular application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.3 Formatting Issues in Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've already seen in the previous section, delimitation in text can be an issue. While it is usually a good idea to go with a reliable tokenizer, sometimes you might indeed have to deal with delimitation and whitespace issues on a more granular level. We can use simply counting the words in a text as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tYou might have an easy time reading this, \n",
      "but the computer has some extra spaces tabs and \n",
      "newlines to deal with.  After all, two spaces after \n",
      "a stop isn't strange!\n",
      "\n",
      "[('spaces', 2), ('\\tYou', 1), ('might', 1), ('have', 1), ('an', 1), ('easy', 1), ('time', 1), ('reading', 1), ('this,', 1), ('\\nbut', 1), ('the', 1), ('computer', 1), ('has', 1), ('some', 1), ('extra', 1), ('tabs', 1), ('and', 1), ('\\nnewlines', 1), ('to', 1), ('deal', 1), ('with.', 1), ('', 1), ('After', 1), ('all,', 1), ('two', 1), ('after', 1), ('\\na', 1), ('stop', 1), (\"isn't\", 1), ('strange!', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "document = \"\\tYou might have an easy time reading this, \\nbut the computer has some extra spaces tabs and \\nnewlines to deal with.  After all, two spaces after \\na stop isn't strange!\"\n",
    "print(document)\n",
    "print()\n",
    "word_counts = Counter()\n",
    "words = re.split(\" \", document)\n",
    "for word in words:\n",
    "    word_counts[word] += 1\n",
    "\n",
    "print(word_counts.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there was irregular whitespace in the text, it ended up attached to some of the words and we also ended up counting an empty string. This is a result of using `re.split()` and specifying a single space character as the delimiter. Using the `string.split()` method is a possible solution, as would be splitting by the more general `\\W` character class (non-word characters). We can also apply the `string.strip()` method to remove extra whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spaces', 2), ('You', 1), ('might', 1), ('have', 1), ('an', 1), ('easy', 1), ('time', 1), ('reading', 1), ('this', 1), ('but', 1), ('the', 1), ('computer', 1), ('has', 1), ('some', 1), ('extra', 1), ('tabs', 1), ('and', 1), ('newlines', 1), ('to', 1), ('deal', 1), ('with', 1), ('After', 1), ('all', 1), ('two', 1), ('after', 1), ('a', 1), ('stop', 1), ('isn', 1), ('t', 1), ('strange', 1)]\n"
     ]
    }
   ],
   "source": [
    "document = \"\\tYou might have an easy time reading this, \\nbut the computer has some extra spaces tabs and \\nnewlines to deal with.  After all, two spaces after \\na stop isn't strange!\"\n",
    "\n",
    "word_counts = Counter()\n",
    "words = re.split(\"\\W\", document)\n",
    "for word in words:\n",
    "    word = word.strip()\n",
    "    if word:\n",
    "        word_counts[word] += 1\n",
    "    \n",
    "print(word_counts.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue here is capitalization, is \"After\" a different word from \"after\"? We could solve this by lowercasing all words before counting them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spaces', 2), ('after', 2), ('you', 1), ('might', 1), ('have', 1), ('an', 1), ('easy', 1), ('time', 1), ('reading', 1), ('this', 1), ('but', 1), ('the', 1), ('computer', 1), ('has', 1), ('some', 1), ('extra', 1), ('tabs', 1), ('and', 1), ('newlines', 1), ('to', 1), ('deal', 1), ('with', 1), ('all', 1), ('two', 1), ('a', 1), ('stop', 1), ('isn', 1), ('t', 1), ('strange', 1)]\n"
     ]
    }
   ],
   "source": [
    "document = \"\\tYou might have an easy time reading this, \\nbut the computer has some extra spaces tabs and \\nnewlines to deal with.  After all, two spaces after \\na stop isn't strange!\"\n",
    "\n",
    "word_counts = Counter()\n",
    "words = re.split(\"\\W\", document)\n",
    "for word in words:\n",
    "    word = word.strip().lower()\n",
    "    if word:\n",
    "        word_counts[word] += 1\n",
    "    \n",
    "print(word_counts.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this might still not be good enough. The contraction \"isn't\" here is broken up into \"isn\" and \"t\", but possibly the best way to tokenize this would be \"is\" and \"n't\", since it is a contraction of those two words. Delimitation issues can become problematic very easily, and there is usually no one catch-all way to fix these problems. In general, attention to detail and good use of regular expressions are really the only way to solve these kinds of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.4 Datetime parsing\n",
    "Date-time information is often found in text, and parsing (extracting and interpreting) this information can be a significant preprocessing task. We can use the `dateutil` module to simplify this (`pip3 install py-dateutil`). The `parser` class included in the module can handle a variety of datetime formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term start date\n",
      "2018-09-24 00:00:00\n",
      "\n",
      "Weekly meeting\n",
      "2018-09-20 12:30:00\n",
      "\n",
      "Thanksgiving holiday begins\n",
      "2018-11-20 22:00:00\n",
      "\n",
      "Christmas day\n",
      "2018-12-25 00:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dateutil.parser as dateparser\n",
    "\n",
    "calendar = {\n",
    "    \"Term start date\" : \"September 24, 2018\",\n",
    "    \"Weekly meeting\" : \"Thursday, 12:30pm\",\n",
    "    \"Thanksgiving holiday begins\" : \"11/20/2018 10pm\",\n",
    "    \"Christmas day\" : \"12/25/2018\"\n",
    "}\n",
    "\n",
    "for event in calendar:\n",
    "    print(event)\n",
    "    print(dateparser.parse(calendar[event]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4.1 The datetime module\n",
    "While `dateutil.parser` knows some common date-time string patterns, these can be more exactly specified for parsing using the `datetime` module:\n",
    "- https://docs.python.org/3/library/datetime.html\n",
    "`datetime` defines Python's temporal objects, and offers a huge number of utilities for working with time, such as an easy funcationality to get the current time of execution (see below). In general, with the `datetime` module dates can be converted to and from numeric objects for calculations to be performed easily and intuitively. Here's an example calculating _timedeltas_, i.e., numeric differences in time between datetime strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to thanksgiving holidays: \n",
      "64 days, 4:08:42.530795\n",
      "\n",
      "Days until Thanksgiving holidays: \n",
      "64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_time = datetime.now()\n",
    "\n",
    "thanksgiving = dateparser.parse(calendar[\"Thanksgiving holiday begins\"])\n",
    "\n",
    "print(\"Time to thanksgiving holidays: \")\n",
    "print((thanksgiving - current_time))\n",
    "print()\n",
    "\n",
    "# if we wanted just the days\n",
    "print(\"Days until Thanksgiving holidays: \")\n",
    "print((thanksgiving - current_time).days)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.4.2 Exercise: Calculate youre exact age\n",
    "Calculate your own age using datetime parsing! Can you come up with a datetime format for your birthday that `dateutil.parser` doesn't recognize or recognizes incorrectly? If so, use the `datetime` module to specify the format exactly. [Hint. Review these docs: \n",
    "- https://docs.python.org/3/library/datetime.html#datetime.datetime.strptime\n",
    "- https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birthday = \"\"\n",
    "\n",
    "# code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
