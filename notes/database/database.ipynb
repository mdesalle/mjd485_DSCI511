{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 511: Data acquisition and pre-processing<br>Chapter 8: Establishing a Database with Documentation\n",
    "\n",
    "## 8.0 From data collection to database construction\n",
    "Supposing we've finished creating our data acquisition script and we can downloaded all kinds of data, are we done?\n",
    "No way! Not only do data scientists need to be experts at acquiring data, they need to also be able to create\n",
    "efficient databases with the data they collect (with appropriate documentation!), analyze said data (not so much in this course), and \n",
    "appropriately communicate their findings. This week we'll be talking about the creation of databases. Maintaining generality, a _database_ for us is simply a collection of data with structure and format intentional for a particular mode of access of interaction.\n",
    "\n",
    "Note: As with other topics in this course, we'll utilize some pretty low-level Python tools instead of any specific (relational, i.e., sql) software. There are extensive courses in the CCI curricula available, so please contact your instructor or other advisors for more information on the college's SQL-specific coursework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Directory structures need to scale!\n",
    "Let's first talk about good habits to get into when actually storing your data. Oftentimes, data science projects will last a long time, and proper organization of the both the code used during the project along with the actual data files themselves is essential to keep data scientists from going insane. Of course, we begin with a new folder for our project. Let's say we're working on a project called `Hello World`. So, we've created our `hello_world` directory and are residing there. It's usually best to keep your code and data separated, so we'd begin by creating appropriate directories, `./hello_world/code` and `./hello_world/data`. The way you organize the `code` folder is entirely dependent on the project. What's more important, and often more difficult, is the organization of the `data` folder. \n",
    "\n",
    "Creation of directories is quick and easy (as is shown below using the `os` module). In order to facilitate the organization of your data, don't be scared of creating extra directories. Sometimes one of the easiest schema for organizing a set of records is to keep each record with its associated metadata in its own directory. This is what is done below in an updated music scraping script. Just make sure when you are trying to retrieve your data to first check if the directory you are trying to access actually exists! To check and see if a file or directory exists we can use the `os.path.exists(path)` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"data/\"))\n",
    "print(os.path.exists(\"database.ipynb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Metadata documents for strategic data interaction\n",
    "First, in case you are unfamiliar with the term, __metadata__ has a bit of a nebulous definition. For our purposes, we will just go with the following basic definition: metadata is a set of data which describes any other set of data. It is very important to set up metadata documentation about datasets, particularly if they are quite large. Imagine if we had a data set of 2 million tweets, and we wanted to find each tweet made by a specific user. If we want to do this without strictly using the tweets themselves, we would need to set up a loop to go through each and every element in the set of 2 million, checking each one for a match. On the other hand, if we set up an additional document which lists the tweet IDs associated with each tweet made per user, then this task becomes trivial. While setting up this document would take a while, the amount of time saved throughout the lifecycle of the project will probably be massive. Why perform the same computations over and over when we can do it once and for all in the beginning of our work? If this wasn't totally clear, that's okay. We'll discuss these concepts in more detail below.\n",
    "\n",
    "### 8.2.1 Inverted Indices\n",
    "An __inverted index__ is an index data structure storing a mapping from content to its locations in a dataset. Below, we'll map the genres of scraped songs to the songs that are a part of the given genres. What's the point of making an inverted index? Is it actually worth the effort? Yes, absolutely!\n",
    "\n",
    "Inverted indices are one of the data structures which allow for search engines like Google to be so fast. Say you search Google for \"data science\". Perhaps unsurprisingly, the search engine doesn't immediately scrape every web page on the internet and search each of them in succession for the query of \"data science\". Instead, Google continually crawls the web and updates a massive inverted index where alongside each search query there exists a list of associated web pages that contain information relevant to the query. This is one of the key components of the modern search engine which allows for such quick results. Here's a good graphic describing the kind of inverted index data uses to resolve keyword search terms:\n",
    "![Inverted Idices](./images/inverted-index.jpg)\n",
    "\n",
    "#### 8.2.1.1 Example: Facebook comments organized by thread\n",
    "Let's look at another context for using inverted indices using some mocked up Facebook comments from a thread. For more information on what comment objects look like on Facebook, check out the docs:\n",
    "- https://developers.facebook.com/docs/graph-api/reference/v3.1/comment\n",
    "\n",
    "Say we have a large corpus of Facebook comments. When the Facebook Graph API is queried, each comment is returned in its own `JSON` object. When loaded into Python using the `json` module, this yields a dictionary of the following form (depending on the requested/accessible values):\n",
    "\n",
    "```\n",
    "{\n",
    "    \"created_time\": \"2018-05-05T21:38:24+0000\",\n",
    "    \"id\": \"10156502712828459_10156502784433450\",\n",
    "    \"message\": \"I love data science!\"\n",
    "}\n",
    "```\n",
    "Each message has a unique `id`, with a twist. It turns out the part of the `id` before the underscore is just the `id` of the Object the comment was made on in the first place, e.g., a page's post. The second half of the `id` distinguishes all the comments on the Objects from one another. So, if we have a huge corpus of Facebook comments and wanted to do some analysis of the various threads, it'd be really useful to create an inverted index that lists each comment that we have for each thread.\n",
    "\n",
    "#### 8.2.1.2 Extended example: building an inverted index to analyzing threads\n",
    "As it turns out, we can think of the first half of the comment ids as metadata, and use them to build an inverted index that allows us to lookup comments by thread. Since we can't publicly access these data, here's some code to 1) create a data directory for this chapter/'project' and mock up Facebook comments data in `'./data/COMMENTS.json'`. \n",
    "\n",
    "To create our directory let's use the following bash command: `mkdir -p <DIRECTORY>`, where the `-p` flag tells `mkdir` to build directories for any path dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"mkdir -p ./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't seen it yet, we cab use the `%%writefile <FILE>` IPython 'magic' command at the top of a cell to create a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/COMMENTS.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/COMMENTS.json\n",
    "[\n",
    "    {\n",
    "        \"created_time\": \"2018-08-21T17:12:22+0000\",\n",
    "        \"id\": \"T2_C2\",\n",
    "        \"message\": \"Yeah, but we can still fake some data up here to get the point across.\"\n",
    "    },\n",
    "    {\n",
    "        \"created_time\": \"2018-05-05T21:38:24+0000\",\n",
    "        \"id\": \"T1_C1\",\n",
    "        \"message\": \"I love data science!\"\n",
    "    },\n",
    "    {\n",
    "        \"created_time\": \"2018-08-21T15:24:16+0000\",\n",
    "        \"id\": \"T2_C1\",\n",
    "        \"message\": \"Unfortunately terms of use can make it difficult to share data.\"\n",
    "    },\n",
    "    {\n",
    "        \"created_time\": \"2018-05-05T22:02:04+0000\",\n",
    "        \"id\": \"T1_C2\",\n",
    "        \"message\": \"Yeah, but I didn't expect all of this pre-processing work!\"\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some 'comments' in place, our goal is to create and use an inverted index file to efficiently access comments by thread. \n",
    "\n",
    "Note: Storing each thread's comments in a separate file would allow us to load the individual threads more quickly. Since the comments would be accessed according to a thread (Post's) ID, this would actually be a convenient way to initially store the data. Moreover, such storage would be a very important step to take if we were going scale our database up, say, continually collecting data. However, storing comments in thread-level files biases access towards thread-level analysis. So, if we were primarily interested in accessing our data by time, e.g., for a timeseries analysis, it would be more convenient to organize the comments into files by day!\n",
    "\n",
    "Regardless, if the database we're setting up is for a fixed amount of content, the code required to set up our thread-access system is a bit simpler. So this is a good place to start! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Get the comments\n",
    "with open('./data/COMMENTS.json', 'r') as f:\n",
    "    COMMENTS = json.load(f)\n",
    "    \n",
    "ids_by_thread = defaultdict(list)\n",
    "comments_by_id = {}\n",
    "\n",
    "for comment in COMMENTS:\n",
    "    threadID, commentID = comment['id'].split('_')\n",
    "    ids_by_thread[threadID].append(commentID)\n",
    "    comments_by_id[comment['id']] = comment\n",
    "    \n",
    "# Write the new files\n",
    "with open('./data/ids_by_thread.json', 'w') as f:\n",
    "    json.dump(ids_by_thread, f, sort_keys = True, indent = 4)\n",
    "    \n",
    "with open('./data/comments_by_id.json', 'w') as f:\n",
    "    json.dump(comments_by_id, f, sort_keys = True, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `ids_by_thread.json` file is a JSON file which yields a relatively small dictionary where the keys are `threadID`s, and the values are lists of the associated `commentID`s. Note: to ease the loading of a thread (or switching between threads) we've also loaded the comments a bit differently this time, storing each comment in a dictionary (nee a list), keyed by `commentID`\n",
    "\n",
    "Now we'll work on some functions that will ease a thread-by-thread analysis. First, let's create a basic Python class to represent a thread, and have it initialize to parses message times and store our comments in their order of appearance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "class Thread(object):\n",
    "    \"\"\"Represents a full Facebook commentary thread. comments is a list of the threads comments\"\"\"\n",
    "    \n",
    "    def __init__(self, comments, IDs):\n",
    "        # Initiate the variables        \n",
    "        self.comments = []\n",
    "        for ID in IDs:\n",
    "            self.comments.append((self.getTime(comments[ID]['created_time']), comments[ID]))\n",
    "        self.comments.sort()\n",
    "                                  \n",
    "    @staticmethod  # This just means the function isn't strictly needed for the class, but useful to have\n",
    "    def getTime(time):\n",
    "        \"\"\"produce a datetime object from a time string\"\"\"\n",
    "        formatted = datetime.datetime.strptime(time, \"%Y-%m-%dT%H:%M:%S+0000\")\n",
    "        return formatted   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we can create some functions that can easily extract useful data for us. First, we create a function that loads up a Thread class instance for us, which the other functions will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(datetime.datetime(2018, 5, 5, 21, 38, 24),\n",
       "  {'created_time': '2018-05-05T21:38:24+0000',\n",
       "   'id': 'T1_C1',\n",
       "   'message': 'I love data science!'}),\n",
       " (datetime.datetime(2018, 5, 5, 22, 2, 4),\n",
       "  {'created_time': '2018-05-05T22:02:04+0000',\n",
       "   'id': 'T1_C2',\n",
       "   'message': \"Yeah, but I didn't expect all of this pre-processing work!\"})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./data/ids_by_thread.json', 'r') as f:\n",
    "    ids_by_thread = json.load(f)\n",
    "    \n",
    "with open('./data/comments_by_id.json', 'r') as f:\n",
    "    comments_by_id = json.load(f)\n",
    "    \n",
    "## just take an arbitrary threads comment IDs\n",
    "threadID = list(ids_by_thread.keys())[0]\n",
    "IDs = [threadID+\"_\"+commentID for commentID in ids_by_thread[threadID]]\n",
    "\n",
    "thread_obj = Thread(comments_by_id, IDs)\n",
    "    \n",
    "thread_obj.comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code might have seemed like a bit of work, but now, for the rest of the lifecycle of our project, we can easily access different threads in our data. For example, since our class structures a given thread by order of appearance, we can now easily iterate through the conversation as it unfolded, with parsed times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-05 21:38:24 I love data science!\n",
      "2018-05-05 22:02:04 Yeah, but I didn't expect all of this pre-processing work!\n"
     ]
    }
   ],
   "source": [
    "for time, comment in thread_obj.comments:\n",
    "    print(time, comment['message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Getting the data right, the first time\n",
    "While accessing a large dataset it might not seem convient to pre-define and execute a database structure, preprocess data, or generate metadata, but it can really save a lot of time on the back end, and should definitely be a prioroty with ongoing (streaming) data collections. Let's revisit out song lyrics exercise and do some strategic preprocessing and file management.\n",
    "\n",
    "#### 8.2.1.1 Exercise: Building a song lyrics database with metadata\n",
    "Let's work with the song lyrics scraper we created during the Harvesting Data lecture. Instead of downloading the entire dataset again and then creating our metadata files, it'd be much more efficient to rewrite our data acquisition procedure to create the metadata as it runs&mdash;upon download, we alread have each piece of data interpreted in memory, i.e.,  don't have to read from disk! The old pieces of web scraping code was just storing songs by artist in large, alphabetic data files. Here, our tasks center around making sure the songs are organized alphabeticaly by artist, and are accessible by albums and genres. We'll want to exercise care as we create data and metadata files&mdash;we need to come up with a consistent naming scheme for the different artists and songs since they don't have IDs from the website&mdash;what could go wrong if we just named files according to artist, album, song, or genre names?\n",
    "\n",
    "Pulling the pieces of scaper code together, here's a fill-in the blanks-style exercise. Complete the marked changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, re, string, json, os\n",
    "\n",
    "#######################################################################\n",
    "####### 0. Create a primary data directory. ###########################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "#######################################################################\n",
    "####### 1. Create reverse-lookup for songs by genre ###################\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "## go through all of the letters in the alphabet\n",
    "for letter in string.ascii_lowercase:\n",
    "    \n",
    "    #######################################################################\n",
    "    ####### 2. Create the letter-level directory ##########################\n",
    "    #######################################################################\n",
    "    #######################################################################\n",
    "    \n",
    "    #######################################################################\n",
    "    ####### 3. Initialize a letter-level metadata file ####################\n",
    "    ####### create a data file for the current letter\n",
    "    filename = \"songlyrics-{}.json\".format(letter)\n",
    "    fh = open(filename,  \"w\")\n",
    "    fh.close()\n",
    "    #######################################################################\n",
    "    #######################################################################\n",
    "    \n",
    "    ## open and parse the html for the current letter\n",
    "    letter_link = 'http://www.songlyrics.com/{}/'.format(letter)\n",
    "    letterhtml = requests.get(letter_link).text\n",
    "    lettersoup = BeautifulSoup(letterhtml, 'html.parser')\n",
    "\n",
    "    ## collect the pages for this letter\n",
    "    pages = [\"/{}/\".format(letter)]\n",
    "    for letterlink in lettersoup.find_all('a'):\n",
    "        ## filter links for letter pages\n",
    "        if letterlink.get(\"href\") and re.search(\"^Page \\d+$\", letterlink.get(\"title\", \"NOTITLE\")):            \n",
    "            pages.append(letterlink['href'])\n",
    "\n",
    "    ## go through the letter pages\n",
    "    for page in pages:        \n",
    "        ## open and parse the html for the current page of this letter\n",
    "        pagehtml = requests.get(\"http://www.songlyrics.com\" + page).text\n",
    "        pagesoup = BeautifulSoup(pagehtml, 'html.parser')\n",
    "\n",
    "        ## go through the artists in the page\n",
    "        for pagelink in pagesoup.find_all('a'):\n",
    "            ## filter links for artist pages\n",
    "            if re.search(\"^http://.*?-lyrics/$\", pagelink.get(\"href\", \"NOLINK\")):\n",
    "\n",
    "                #######################################################################                \n",
    "                ####### 4. remove old data structure and hold on to the artist's name \n",
    "                ####### set up data and store artist-level information\n",
    "                data = {\n",
    "                    \"Artist\": pagelink.text,\n",
    "                    \"url\": pagelink['href'],\n",
    "                    \"Songs\": {}\n",
    "                }\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "\n",
    "                #######################################################################\n",
    "                ####### 5. Output artist info to letter-level metadata file ###########\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "\n",
    "                #######################################################################\n",
    "                ####### 6. Create artist-level directory. #############################\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "                \n",
    "                #######################################################################\n",
    "                ####### 7. Create an artist-level metadata file #######################\n",
    "                #######################################################################\n",
    "                #######################################################################      \n",
    "                \n",
    "                ## open and parse the html for the current artist on this page\n",
    "                artisthtml = requests.get(data[\"url\"]).text\n",
    "                artistsoup = BeautifulSoup(artisthtml, 'html.parser')                        \n",
    "\n",
    "                ## go through the songs of this artist\n",
    "                for songlink in artistsoup.find_all('a'):\n",
    "\n",
    "                    ## filter links for song pages\n",
    "                    if songlink.get(\"itemprop\", \"NOITEMPROP\") == \"url\" and songlink.get(\"title\"):\n",
    "                                                \n",
    "                        #######################################################################\n",
    "                        ############ 8. Hold song title; store info as artist-level metadata\n",
    "                        ############ store initial song-level information\n",
    "                        title = songlink.text\n",
    "                        data[\"Songs\"][title] = {\"Title\": title}\n",
    "                        data[\"Songs\"][title][\"url\"] = songlink['href']\n",
    "                        #######################################################################\n",
    "                        #######################################################################                        \n",
    "\n",
    "                        ## open and parse the html for the current song by this artist\n",
    "                        songhtml = requests.get(data[\"Songs\"][title][\"url\"]).text\n",
    "                        songsoup = BeautifulSoup(songhtml, 'html.parser')\n",
    "\n",
    "                        ## go through paragraphs to find song attributes\n",
    "                        for par in songsoup.find_all(\"p\"):\n",
    "                            if re.search(\": \", par.text):\n",
    "                                pieces = re.split(\": \", par.text)\n",
    "                                key = pieces[0]\n",
    "                                value = \": \".join(pieces[1: len(pieces)])\n",
    "\n",
    "                                #######################################################################                                \n",
    "                                ############ 9. add song attributes to artist-level metadata ##########\n",
    "                                data[\"Songs\"][title][key] = value    \n",
    "                                #######################################################################\n",
    "                                #######################################################################                        \n",
    "\n",
    "                                #######################################################################                                \n",
    "                                ############ 10. add song attributes to reverse song lookup ###########\n",
    "                                #######################################################################\n",
    "                                #######################################################################                                \n",
    "\n",
    "                        #######################################################################                                \n",
    "                        ############ 11. output song metadata to artist-level metadata file ###\n",
    "                        #######################################################################\n",
    "                        #######################################################################                                \n",
    "                                \n",
    "                        ## go through divs to find the one with the song lyrics\n",
    "                        for div in songsoup.find('body').find_all('div'):\n",
    "                            if div.get(\"id\",\"NOCLASS\") == \"songLyricsDiv-outer\":\n",
    "                                \n",
    "                                #######################################################################                                \n",
    "                                ############ 12. output song lyrics as text in artist-level directory #                                \n",
    "                                data[\"Songs\"][title][\"Lyrics\"]=div.text\n",
    "                                #######################################################################\n",
    "                                #######################################################################\n",
    "                        \n",
    "                        break\n",
    "                        \n",
    "                #######################################################################\n",
    "                #### 13. remove old data write out ####################################\n",
    "                \n",
    "                ## write out the data for this artist, appending to the end of this letter's file\n",
    "                with open(filename, \"a\") as fh:\n",
    "                    fh.writelines(json.dumps(data)+\"\\n\")\n",
    "                    \n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "                \n",
    "                break\n",
    "        break\n",
    "        \n",
    "    break\n",
    "\n",
    "#######################################################################\n",
    "####### 14. Output reverse-lookup for songs by attributes #############\n",
    "#######################################################################\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.1.2 Solution: Making the changes (Spoilers if you're doing the exercise!)\n",
    "By making a reverse lookup metadata file for artists by associated song attributes, we are denormalizing data and making it easy to perform specific transformations which are interesting for analysis. So here is the above scraper, with the additional changes we wanted to make all filled out. This is a lot, so make sure to take some time and really figure out what's going on here. If you run this code, take the time as well to review the directory structure and files it creates, and if you performed the above exercise on your own, compare your edits to those below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests, re, string, json, os\n",
    "\n",
    "#######################################################################\n",
    "####### 0. Create a primary data directory. ###########################\n",
    "os.system(\"mkdir ./data/\")\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "#######################################################################\n",
    "####### 1. Create objects for reverse-lookup of songs by genre ########\n",
    "songsByAttribute = {}\n",
    "attributeIDs = {}\n",
    "attributes = {}\n",
    "attributeNumbers = {}\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "\n",
    "## go through all of the letters in the alphabet\n",
    "for letter in string.ascii_lowercase:\n",
    "\n",
    "    numartists = 0\n",
    "    \n",
    "    #######################################################################\n",
    "    ####### 2. Create the letter-level directory ##########################\n",
    "    os.system(\"mkdir ./data/{}/\".format(letter))\n",
    "    #######################################################################\n",
    "    #######################################################################    \n",
    "    \n",
    "    #######################################################################\n",
    "    ####### 3. Initialize a letter-level metadata file ####################\n",
    "    lettermetafile = \"./data/{}/lettermeta.json\".format(letter)\n",
    "    fh = open(lettermetafile,  \"w\")\n",
    "    fh.close()    \n",
    "    #######################################################################\n",
    "    #######################################################################\n",
    "    \n",
    "    ## open and parse the html for the current letter\n",
    "    letter_link = 'http://www.songlyrics.com/{}/'.format(letter)\n",
    "    letterhtml = requests.get(letter_link).text\n",
    "    lettersoup = BeautifulSoup(letterhtml, 'html.parser')\n",
    "\n",
    "    ## collect the pages for this letter\n",
    "    pages = [\"/{}/\".format(letter)]\n",
    "    for letterlink in lettersoup.find_all('a'):\n",
    "        ## filter links for letter pages\n",
    "        if letterlink.get(\"href\") and re.search(\"^Page \\d+$\", letterlink.get(\"title\", \"NOTITLE\")):            \n",
    "            pages.append(letterlink['href'])\n",
    "\n",
    "    ## go through the letter pages\n",
    "    for page in pages:        \n",
    "        ## open and parse the html for the current page of this letter\n",
    "        pagehtml = requests.get(\"http://www.songlyrics.com\" + page).text\n",
    "        pagesoup = BeautifulSoup(pagehtml, 'html.parser')\n",
    "\n",
    "        ## go through the artists in the page\n",
    "        for pagelink in pagesoup.find_all('a'):\n",
    "            ## filter links for artist pages\n",
    "            if re.search(\"^http://.*?-lyrics/$\", pagelink.get(\"href\", \"NOLINK\")):\n",
    "                \n",
    "                #######################################################################                \n",
    "                ####### 4. remove old data structure and hold on to the artist's data #\n",
    "                ####### keep track of number of artists, songs, and create an ID\n",
    "                numartists += 1\n",
    "                artistID = \"{}-{}\".format(letter, str(numartists))\n",
    "                numsongs = 0\n",
    "                \n",
    "                artist = pagelink.text\n",
    "                artisturl = pagelink['href']\n",
    "                \n",
    "                \n",
    "#                 ## set up data and store artist-level information\n",
    "#                 data = {\n",
    "#                     \"Artist\": pagelink.text,\n",
    "#                     \"url\": pagelink['href'],\n",
    "#                     \"Songs\": {}\n",
    "#                 }                \n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "\n",
    "                #######################################################################                \n",
    "                ####### 5. Output artist info to letter-level metadata file ###########\n",
    "                with open(lettermetafile,  \"a\") as f:\n",
    "                    f.writelines(artistID + \"\\t\" + artist + \"\\t\" + artisturl + \"\\n\")                    \n",
    "                #######################################################################\n",
    "                #######################################################################                    \n",
    "                    \n",
    "                #######################################################################\n",
    "                ####### 6. Create artist-level directory. #############################\n",
    "                artist_dir = './data/{}/{}/'.format(letter, artistID)\n",
    "                os.system(\"mkdir \" + artist_dir)\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "                \n",
    "                #######################################################################\n",
    "                ####### 7. Create an artist-level metadata file #######################\n",
    "                artistmetafile = artist_dir + \"artistmeta.json\"\n",
    "                fh = open(artistmetafile,  \"w\")\n",
    "                fh.close()               \n",
    "                #######################################################################\n",
    "                #######################################################################                \n",
    "                                \n",
    "                ## open and parse the html for the current artist on this page\n",
    "                ## note we now use the artist's url!\n",
    "                artisthtml = requests.get(artisturl).text\n",
    "                artistsoup = BeautifulSoup(artisthtml, 'html.parser')                        \n",
    "\n",
    "                ## go through the songs of this artist\n",
    "                for songlink in artistsoup.find_all('a'):\n",
    "\n",
    "                    ## filter links for song pages\n",
    "                    if songlink.get(\"itemprop\", \"NOITEMPROP\") == \"url\" and songlink.get(\"title\"):                        \n",
    "\n",
    "                        #######################################################################\n",
    "                        ############ 8. Hold song title; store info as artist-level metadata ##\n",
    "                        ## keep track of number of songs and create and ID\n",
    "                        numsongs += 1\n",
    "                        titleID = \"{}-{}\".format(artistID, str(numsongs))\n",
    "                        \n",
    "                        ## hold on to the song's title\n",
    "                        title = songlink.text\n",
    "                        \n",
    "#                         data[\"Songs\"][title] = {\"Title\": title}\n",
    "#                         data[\"Songs\"][title][\"url\"] = songlink['href']\n",
    "\n",
    "                        data = {\n",
    "                            \"ID\": titleID,\n",
    "                            \"title\": title,\n",
    "                            \"url\": songlink['href']\n",
    "                        }\n",
    "                        #######################################################################\n",
    "                        #######################################################################\n",
    "\n",
    "                        ## open and parse the html for the current song by this artist\n",
    "                        ## note the data format has changed to get the song's url!\n",
    "                        songhtml = requests.get(data[\"url\"]).text\n",
    "                        songsoup = BeautifulSoup(songhtml, 'html.parser')\n",
    "\n",
    "                        ## go through paragraphs and get song attributes\n",
    "                        for par in songsoup.find_all(\"p\"):\n",
    "                            if re.search(\": \", par.text):\n",
    "                                pieces = re.split(\": \", par.text)\n",
    "                                key = pieces[0]\n",
    "                                value = \": \".join(pieces[1: len(pieces)])\n",
    "\n",
    "                                #######################################################################                                \n",
    "                                ############ 9. add song attributes to artist-level metadata ##########\n",
    "                                if key != \"Note\":\n",
    "                                    data[key] = value\n",
    "                                #######################################################################\n",
    "                                #######################################################################\n",
    "                                \n",
    "                                #######################################################################                                \n",
    "                                ############ 10. add song attributes to reverse song lookup ###########\n",
    "                                if key != \"Note\":\n",
    "                                    attributeNumbers.setdefault(key, 1)\n",
    "                                    attributeIDs.setdefault(key, {})\n",
    "                                    attributes.setdefault(key, {})\n",
    "                                    if not attributeIDs[key].get(value, False):\n",
    "                                        attributeID = \"{}-{}\".format(key, str(attributeNumbers[key]))\n",
    "                                        attributes[key][attributeID] = value\n",
    "                                        attributeIDs[key][value] = attributeID\n",
    "                                        attributeNumbers[key] += 1\n",
    "                                    else:\n",
    "                                        attributeID = attributeIDs[key][value]                                        \n",
    "                                    \n",
    "                                    songsByAttribute.setdefault(key, {})\n",
    "                                    songsByAttribute[key].setdefault(attributeID, {})\n",
    "                                    songsByAttribute[key][attributeID].setdefault(artistID, [])\n",
    "                                    songsByAttribute[key][attributeID][artistID].append(titleID)\n",
    "                                #######################################################################\n",
    "                                #######################################################################\n",
    "\n",
    "                        #######################################################################                                \n",
    "                        ############ 11. output song metadata to artist-level metadata file ###\n",
    "                        with open(artistmetafile,  \"a\") as f:\n",
    "                            f.writelines(json.dumps(data) + \"\\n\")\n",
    "                        #######################################################################\n",
    "                        #######################################################################                            \n",
    "                                \n",
    "                        ## go through divs to find the one with the song lyrics\n",
    "                        for div in songsoup.find('body').find_all('div'):\n",
    "                            if div.get(\"id\", \"NOCLASS\") == \"songLyricsDiv-outer\":\n",
    "\n",
    "                                #######################################################################                                \n",
    "                                ############ 12. output song lyrics as text in artist-level directory #\n",
    "                                title_file = \"./data/{}/{}/{}.txt\".format(letter, artistID, titleID)\n",
    "                                with open(title_file, \"w\") as f:\n",
    "                                    f.writelines(div.text + \"\\n\")\n",
    "                                \n",
    "#                                 data[\"Songs\"][title][\"Lyrics\"]=div.text\n",
    "                                #######################################################################\n",
    "                                #######################################################################\n",
    "\n",
    "                                break\n",
    "            \n",
    "                    ## now, only break after 10 songs by an artist\n",
    "                    if numsongs >= 1:\n",
    "                        break\n",
    "                        \n",
    "                #######################################################################\n",
    "                #### 13. remove old data write out ####################################\n",
    "#                 ## write out the data for this artist, appending to the end of this letter's file\n",
    "#                 with open(filename, \"a\") as fh:\n",
    "#                     fh.writelines(json.dumps(data)+\"\\n\")\n",
    "                #######################################################################\n",
    "                #######################################################################\n",
    "                \n",
    "            ## now, only break if this is the tenth artist of this letter!\n",
    "            if numartists >= 1:\n",
    "                break\n",
    "        \n",
    "        ## this stops us after one page of each letter\n",
    "        break\n",
    "        \n",
    "    ## this stops us after one letter in the alphabet\n",
    "#     break\n",
    "\n",
    "#######################################################################\n",
    "####### 14. Output reverse-lookup for songs by genre ##################\n",
    "os.system(\"mkdir ./data/Genre/\")\n",
    "fh = open(\"./data/Genre/attributeIDs.txt\", \"w\")\n",
    "for attributeID in songsByAttribute[\"Genre\"]:\n",
    "    fh.writelines(attributeID + \"\\t\" + attributes[\"Genre\"][attributeID] + \"\\n\")\n",
    "    with open(\"./data/Genre/\" + attributeID + \".json\", \"w\") as f:\n",
    "        f.writelines(json.dumps(songsByAttribute[\"Genre\"][attributeID]) + \"\\n\")\n",
    "fh.close()\n",
    "#######################################################################\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Accessing our database\n",
    "The goal here is to create API-like access for our local database. Let's work on our goal of being able to access data by genre. To do this, we'll make a function that reads the appropriate reverse-lookup file and finds all songs titles/artists with the desired genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = {}\n",
    "with open(\"./data/Genre/attributeIDs.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        ID, genre = re.split(\"\\t\", line)\n",
    "        genres[genre] = ID\n",
    "\n",
    "def genreSongs(genre):\n",
    "    with open(\"./data/Genre/{}.json\".format(genres[genre]), 'r') as f:\n",
    "        genredata = json.load(f)\n",
    "    data = []\n",
    "    for artistID in genredata:\n",
    "        letter = artistID[0]\n",
    "        songs = genredata[artistID]\n",
    "        with open(\"./data/{}/{}/artistmeta.json\".format(letter, artistID)) as f:\n",
    "            for line in f:\n",
    "                songmeta = json.loads(line.strip())\n",
    "                if songmeta[\"ID\"] in songs:\n",
    "                    data.append((songmeta[\"Artist\"], songmeta[\"title\"]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist: A\n",
      "Song: Sing-A-Long\n",
      "Artist: X\n",
      "Song: 4th of July\n"
     ]
    }
   ],
   "source": [
    "for artist, song in genreSongs(\"Rock\"):\n",
    "    print(\"Artist: \" + artist)\n",
    "    print(\"Song: \" + song)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2.2.1 Exercise: accessing songs by album\n",
    "Review the `genreSongs()` function and use it as a starting point to retrieve a specific albumn's worth of song data for a specified artist. Albums and artists should be specified by string arguments. Be sure to have this functionfail gracefully/informatively if no match is found in the database! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## place code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
