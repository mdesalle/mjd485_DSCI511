{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 511: Data acquistion and pre-processing<br>Chapter 9: Distribution, accessibility, and data sharing\n",
    "\n",
    "## 9.0 Can we share?\n",
    "What to do with an exciting, completed dataset? Well, if you're asking this question you've probably already released the data to a group or individual downstream who motivated the dataset's development. But datasets often have unknown value outside of their impetuses for development and moreover, if you're in the business of developing dataset then you probably would like to create some visibility for your work, much like a porfolio. However, before addressing to take these final steps let's approach a few questions:\n",
    "+ What are the reasons for sharing data?\n",
    "+ Is the data in the correct form to be shared?\n",
    "+ Who has the rights to share?\n",
    "+ What mechanism can we use to share?\n",
    "\n",
    "Of course, each group or individual is going to have wildly different answers to these questions. There won't really be any hard and fast rules to stick by (apart from not doing anything illegal!). Probably the best way to proceed is by looking at some examples.\n",
    "\n",
    "## 9.1 A BuzzFeed dataset\n",
    "Let's look at the dataset discussed in the following BuzzFeed article: \n",
    "- https://www.buzzfeednews.com/article/craigsilverman/partisan-fb-pages-analysis.\n",
    "\n",
    "For this, a team of journalists analyzed every post made by 9 separate Facebook pages for news outlets during a 7 business day period for veracity. In other words, each of these posts was read by the journalists and it was determined how factual they were. The journalists assigned the following four veracity categories:\n",
    "+ mostly true\n",
    "+ mostly false\n",
    "+ mixture of true and false\n",
    "+ no factual content\n",
    "\n",
    "The time period chosen was during September 2016 at the height of the U.S. Presidential Election. While the BuzzFeed article reported some interesting analyses of the posts and the attention they received on Facebook, there's likely more interesting analyses that could be performed by others! However, what data from Facebook can be shared? Even if Facebook's data could not be passed forward, it's essential that BuzzFeed's analysis be reproducible! What options exist?\n",
    "\n",
    "### 9.1.1 Leaving a paper trail\n",
    "Let's check out how they actually provided the data that backs up the story. Some searching of the article yields a link to a GitHub repository: \n",
    "- https://github.com/BuzzFeedNews/2016-10-facebook-fact-check \n",
    "\n",
    "which is one of the best places to put data you want the public to see! In addition to being a public-facing venue in which (relatively small) data can be placed, github' focus on version control makes updates and tracking easy. Let's take a look at the first couple of rows of the data in the repository, appearing under the path: \n",
    "- `data/facebook-fact-check.csv`. \n",
    "\n",
    "Note: to access a single file from a github repo, raw content can be found under urls like\n",
    "- https://raw.githubusercontent.com/USERNAME/REPOSITORY/BRANCH/PATH/TO/FILENAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os\n",
    "os.system(\"mkdir -p data\")\n",
    "\n",
    "response = requests.get(\"https://raw.githubusercontent.com/BuzzFeedNews/2016-10-facebook-fact-check/master/data/facebook-fact-check.csv\")\n",
    "\n",
    "with open(\"data/facebook-fact-check.csv\", \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the header and first few lines, what content did BuzzFeed likely control the rights of? Where's the Facebook data? \n",
    "\n",
    "Well, there's actually a few pieces of data coming from Facebook that we should discuss, but most importantly (for tracking and rereproducability) are the ids: `'account_id'` and `'post_id'`. Together, these identify the account owners of the posts and the posts rated, themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account_id', 'post_id', 'Category', 'Page', 'Post URL', 'Date Published', 'Post Type', 'Rating', 'Debate', 'share_count', 'reaction_count', 'comment_count']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "reader = csv.reader(open(\"data/facebook-fact-check.csv\", \"r\"))\n",
    "header = reader.__next__()\n",
    "data = list(reader)\n",
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['184096565021911',\n",
       "  '1035057923259100',\n",
       "  'mainstream',\n",
       "  'ABC News Politics',\n",
       "  'https://www.facebook.com/ABCNewsPolitics/posts/1035057923259100',\n",
       "  '2016-09-19',\n",
       "  'video',\n",
       "  'no factual content',\n",
       "  '',\n",
       "  '',\n",
       "  '146',\n",
       "  '15'],\n",
       " ['184096565021911',\n",
       "  '1035269309904628',\n",
       "  'mainstream',\n",
       "  'ABC News Politics',\n",
       "  'https://www.facebook.com/ABCNewsPolitics/posts/1035269309904628',\n",
       "  '2016-09-19',\n",
       "  'link',\n",
       "  'mostly true',\n",
       "  '',\n",
       "  '1',\n",
       "  '33',\n",
       "  '34']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2 Why publish pointers to data with ids? \n",
    "As it turns out, BuzzFeed published little information from the Facebook platform or the content of the articles, themselves. By attaching their (fully owned) veracity ratings of the articles they made their analysis reproducible and essentially enriched a bunch of data from Facebook. As it turns out, sharing ids from content objects on online platforms has become something of a standard protocol for communication around data. If an individual who wishes to reproduce an experiment or otherwise use its data has access rights, shared ids along with a functional API make it possible to programmatically reconstitute a dataset. Of course, these protocols are different from platform to platform, but it's usually how things work. But what about the share, reaction, and comment counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3 Why not publish the whole dataset?\n",
    "Well, there's a few reasons for this, first and formost many platforms are in the business of monetizing the data they generate. So if you are privilaged enough to build an exciting dataset using proprietary data this is usually the reason for not being allowed. However, there are good other reasons! \n",
    "\n",
    "As it turns out, BuzzFeed arguably may have done better to leave off some data, in particular the last three columns: share, reaction, and comment counts. If these were accessed directly from Facebook's API, then they almost certainly violate our previous reason for non-distribution, but there's an additional problem: when did BuzzFeed publish the data, and does its content currently accurately reflect the state of the posts? The answer is most certainly \"no\". Comments, likes, and reactions, etc., are generally continually modifiable on a public post and now, several years later, are almost certainly out of date. \n",
    "\n",
    "So, do the do comment, etc., counts have no value in BuzzFeed dataset? Well, no! As faithful representatives of a particular post's social attention these comments certainly are out of date, but their placement in the BuzzFeed dataset does indeed satisfy a crucial role. BuzzFeed might hope that their record will have value in reuse, but most importantly they are _documenting_ their work for reproducability. In other words, they have posted the minimal data necessary to be able to reproduce their analysis. Sure, the out of date values might be misleading if they are not represetnted appropriately, but that's a different matter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.4 Identifiable, publicly visable, and reproducible, but accessible?\n",
    "Well, the posts and other data on Facebook used to be. Prior to 2018, Facebook's Graph API allowed un-reviewed apps to collect _public_ data from their Pages API (the posts were mader to Facebook Public Pages). Following the platform's media trouble early that year, Facebook clamped down on data for apps into a need-to-know protocol, i.e., apps now have to apply for access to specific streams with intended uses documented. So while it was possible in the past for independent researchers to [access this data](https://www.aaai.org/ocs/index.php/ICWSM/ICWSM18/paper/view/17825), the only way API users with could access now is using a reviewed app authorized by each of the (9) page owners!\n",
    "\n",
    "This example provides a non-trivial view into data sharing across tightly controlled data; let's look at how this varies and some of the other challenges out there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Variations in accessibility\n",
    "Ok, so we've gone over examples which concern data from fairly restricted sources. Of course, not all data is protected like this (not even all social media data is guarded). Some sources simply have no protections, while other go so far as to encourage anyone and everyone to download their data and play around with it. Let's first look at a key example on electronic texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1 Project Gutenberg, Google Books, and N-grams\n",
    "We've already looked at the Project Gutenberg data, but it's so useful that it's worth revisiting. Basically, it's a large project whose goal is to make books that are currently in the public domain easily accessible to the public at large. But what does it mean for a book to be in the public domain? It turns out that in many countries, copyrights on intellectual property only last for a certain number of years (around 70), after which the work in question loses any protections and becomes available for any conceivable use, without restriction. When it becomes 2019, a collection of books copyrighted in 1923 will become part of the public domain, and henceforth added to Project Gutenberg. The curators at Project Gutenberg take these public domain works and make them available in convenient filetypes (including the appropriate filetypes for e-readers). If you enjoy reading classic literature, this is the place to be online! But even further than that, the project provides a huge reserve of textual data for researchers to play with. \n",
    "\n",
    "While Project Gutenberg only uses books which are in the public domain, Google Books certainly does not. Perhaps at some point you've searched for a sample of a book online to examine before actually purchasing it, chances are you found an entry for the book on Google Books. Only a limited number of pages and even sections are available for viewing of copyrighted books on Google Books. This makes sense, but how can they share even this? Well, for just the reason described above. Showing bits and pieces of a book can often be enough to entince someone to purchase it. So how did Google compile such a huge collection of (partial) books? With close relationships with several of the nation's largest libraries, Google has transfered huge quantities of physical textbooks to a digital form in an effort to preserve records, and allow for analysis. Of course, Google can't be allowed to openly share these reserves of data, as that could collapse the book industry. So, as a kind of happy medium (between not breaking copyright laws and still providing data to potential researchers), Google has used the data obtained from digitizing the massive collections of books to create a database of n-grams. \n",
    "\n",
    "_N-grams_ are basically just snippets of text that contain N words. So, for example, a 2-gram from the previous sentence might be 'text that'. N-gram `features` have become somewhat important in Natural Language Processing and linguistics. Google analyzed its massive trove of text from the books it digitized and published all n-grams and their occurence rates in a huge dataset. This is interesting because the data technically comes from and really is copyrighted material, but presented in a way that the book industry is fine with. This is a good example where context is extremely important. You can check out a neat tool that allows you to play with the dataset here: \n",
    "\n",
    "- https://books.google.com/ngrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2 Wikipedia\n",
    "Wikipedia is famous for its openness. It began with the novel idea of an encyclopedia that anyone could edit, and has now ballooned into the largest information source of its kind on the internet. The vast majority of content is totally open and free on Wikipedia, and they actively encourage users to obtain it. If you dig deeper into Wikipedia's policies, you might think this is incorrect, since they seem to be strictly anti-scraping, as they explicitly state \"Please do not use a web crawler to download large numbers of articles. Aggressive crawling of the server can cause a dramatic slow-down of Wikipedia\". While this is the case, it's only so because Wikipedia provides links to download the entirety of the website all at once! Going to \n",
    "\n",
    "- https://dumps.wikimedia.org/enwiki/ \n",
    "\n",
    "will allow you to download all of the current English-language Wikipedia page, which amounts to about 58 GB of data. There are also ways to download only articles from a specific category, or even a list of specific articles: \n",
    "\n",
    "- https://en.wikipedia.org/wiki/Special:Export\n",
    "- https://www.mediawiki.org/wiki/Manual:Parameters_to_Special:Export\n",
    "\n",
    "Wouldn't it be nice if all data sources had the same policies as Wikipedia?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3 The Yelp Dataset(s)\n",
    "One of the best single datasets for budding data scientists to both test their skills and possibly show off is the Yelp dataset. Every year, Yelp hosts challenges centered around its datasets geared towards students and beginning data scientists. It selects winners based on a whole assortment of criteria. There is much prestige to be had in winning a Yelp dataset challenge (and a potential $5000 prize)! The current challenge includes tasks for photo classification, natural language processing and sentiment analysis, and graph mining. Even if you don't win any of the contests, using this dataset for work is a great idea because it's massive and extremely clean (hardly any munging or preprocessing is required, Yelp does a fantastic job of doing that for you). This is a notorious dataset, and hundreds of academic papers have already been written using it as a basis. \n",
    "\n",
    "#### 9.2.3.1 Anonymous, but precise and complete\n",
    "An interesting point regarding the dataset is that it's extremely complete for a dataset coming from a large social media platform. Really the only aspect of the data that has been altered to fit in with the challenge is the fact that user-identifying information has been scrubbed. Rather than provide the real user identities, Yelp has created randomized unique identities for each user and business&mdash;we'll refer to these as _anonymized_ ids. These allows for researchers to still perform user-level analyses, while avoiding some ethical and privacy concerns. Let's look at a sample of this data! Note: you can find the data at: \n",
    "\n",
    "- https://www.yelp.com/dataset/challenge. \n",
    "\n",
    "The dataset is enormous, so let's just look at a single review pasted into the notebook (tsk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pizza was okay. Not the best I've had. I prefer Biaggio's on Flamingo \\/ Fort Apache. The chef there can make a MUCH better NY            style pizza. The pizzeria @ Cosmo was over priced for the quality and lack of personality in the food. Biaggio's is a much better pick if             youre going for italian - family owned, home made recipes, people that actually CARE if you like their food. You dont get that at a pizzeria          in a casino. I dont care what you say...\n"
     ]
    }
   ],
   "source": [
    "review_example = {\"review_id\": \"x7mDIiDB3jEiPGPHOmDzyw\",\n",
    "                  \"user_id\": \"msQe1u7Z_XuqjGoqhB0J5g\",\n",
    "                  \"business_id\": \"iCQpiavjjPzJ5_3gPD5Ebg\",\n",
    "                  \"stars\": 2,\n",
    "                  \"date\": \"2011-02-25\",        \n",
    "                  \"text\": \"The pizza was okay. Not the best I've had. I prefer Biaggio's on Flamingo \\/ Fort Apache. The chef there can make a MUCH better NY            style pizza. The pizzeria @ Cosmo was over priced for the quality and lack of personality in the food. Biaggio's is a much better pick if             youre going for italian - family owned, home made recipes, people that actually CARE if you like their food. You dont get that at a pizzeria          in a casino. I dont care what you say...\",\n",
    "                  \"useful\": 0, \"funny\": 0, \"cool\": 0}\n",
    "\n",
    "print(review_example[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fully integrate the reviews with the dataset Yelp has actually had to anonymize three identifiers: `review_id` (a unique identifier for each review), `user_id` (an id for the user who left the review), and `business_id` (an id for the business which is being reviewed). Aside from this, the actual content consists of the `text` (body text of the review),`stars` (the number of stars the user rated the business, from 1 to 5), and the `useful`/`funny`/`cool` are ratings (other users reactions to a given review).\n",
    "\n",
    "This file naturally links to out to information about the users, contained in `user.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Susan\n"
     ]
    }
   ],
   "source": [
    "user_sample = {\"user_id\":\"lzlZwIpuSWXEnNS91wxjHw\",\n",
    "               \"name\":\"Susan\",\"review_count\":1,\n",
    "               \"yelping_since\":\"2015-09-28\",\n",
    "               \"friends\":\"None\",\"useful\":0,\"funny\":0,\"cool\":0,\n",
    "               \"fans\":0,\"elite\":\"None\",\"average_stars\":2.0,\n",
    "               \"compliment_hot\":0,\"compliment_more\":0,\"compliment_profile\":0,\"compliment_cute\":0, \n",
    "               \"compliment_list\":0,\"compliment_note\":0,\"compliment_plain\":0,\"compliment_cool\":0,\n",
    "               \"compliment_funny\":0,\"compliment_writer\":0,\"compliment_photos\":0}\n",
    "print(user_sample[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which shows us just how personal some of the information is for each user! Note that while this detail exists about the users and the anonymized ids given by the `user_id` key match to those in the `reviews.json` file, the user's Yelp identity is technically still unknown, as their `user_id` cannot be linked to any existing on their API! \n",
    "\n",
    "So, this gives us some metadata about the users themselves, with information such as how many reviews they've made in total and the average star rating they give businesses. The last piece of the anonymized puzzles is the businesses themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.0918130155 -114.031674872\n"
     ]
    }
   ],
   "source": [
    "business_example = {\"business_id\":\"Apn5Q_b6Nz61Tq4XzPdf9A\",\n",
    "                   \"name\":\"Minhas Micro Brewery\",\"neighborhood\":\"\",\n",
    "                   \"address\":\"1314 44 Avenue NE\",\"city\":\"Calgary\",\n",
    "                   \"state\": \"AB\",\"postal_code\":\"T2E 6L6\",\n",
    "                   \"latitude\": 51.0918130155,\"longitude\": -114.031674872,\n",
    "                   \"stars\": 4.0,\"review_count\": 24, \"is_open\": 1,\n",
    "                   \"attributes\": {\"BikeParking\": False,\"BusinessAcceptsCreditCards\":True,\n",
    "                                  \"BusinessParking\":{'garage': False, 'street': True, \n",
    "                                                     'validated': False, 'lot': False, 'valet': False},\n",
    "                                  \"GoodForKids\": True, \"HasTV\": True, \"NoiseLevel\":\"average\",\n",
    "                                  \"OutdoorSeating\": False,\"RestaurantsAttire\":\"casual\",\n",
    "                                  \"RestaurantsDelivery\": False,\"RestaurantsGoodForGroups\": True,\n",
    "                                  \"RestaurantsPriceRange2\": \"2\",\"RestaurantsReservations\": True,\n",
    "                                  \"RestaurantsTakeOut\": True}, \n",
    "                   \"categories\": \"Tours, Breweries, Pizza, Restaurants, Food, Hotels & Travel\",\n",
    "                   \"hours\": {\"Monday\": \"8:30-17:0\", \"Tuesday\": \"11:0-21:0\", \"Wednesday\": \"11:0-21:0\",\n",
    "                            \"Thursday\": \"11:0-21:0\", \"Friday\": \"11:0-21:0\", \"Saturday\": \"11:0-21:0\"}}\n",
    "print(business_example[\"latitude\"], business_example[\"longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we get the information regarding the businesses that have been reviewed throughout the dataset by the anonymous reviewers. Even though the `\"business_id\"` field is probably anonymized, too, we get a plethora of identifying information about the business&mdash;what would you use to identify this business in the real world? Does obfuscating the true `\"business_id\"` help support user anonymity in any way?\n",
    "\n",
    "In general, the relative non-anonymization of business features makes sense as that's the entire purpose of Yelp; potentially anonymous reviewers can leave honest and frank public reviews of known businesses. Plus, the businesses by nature are most likely interested in being identified (as long as their reviews are good)! It is interesting to note the large amount of information present in these JSONs. This is basically everything you can find out about a business by checking out its Yelp page! Considering these attempts at anonymization and the nature of data being challenging to really separate from identity segways us into some thoughts on how anonymization of personal data varies.\n",
    "\n",
    "### 9.2.4 Levels of personal data\n",
    "You may have noticed that the platform in which data is coming from makes a huge difference on the quantity of identifying information and features present. Some platforms are extremely easy to work with because there's a plethora of data provided, while others are difficult because of how cryptic they are. It's important when planning out a project to be acutely aware of just how much personalized information will be available in the data. Let's try and group this world into three general categories for personal data availability in harvested data:\n",
    "\n",
    "+ Totally anonymized - Although completely anonymous data is becoming more and more rare with the exponential growth of social media platforms, it still exists. A good example would be the family of so-called _chan_ boards (i.e. 4chan, 8chan, etc.). 4chan in particular has a [public API](https://github.com/4chan/4chan-API) that makes it incredibly easy to download any kind of data you'd like from the page, but said data is completely lackling in identifying information. In case you're unfamiliar, 4chan is an imageboard where people post pictures, and then users can leave comments on them. There are no usernames on 4chan, everyone is simply known as _anonymous_. So, while 4chan data is exceedingly easy to acquire and plentiful, there's no way to identify who said what on the platform.\n",
    "+ Semi-anonymized - This is where a lot of the data we've been talking about today would fall. Data in this category is voluntary and potentially non-reliable (from an identification point of view), as a given user can easily assume multiple platform identities. Examples here include the Yelp and Twitter where there may be personally-identifying information, but only if the user, e.g., chooses to use their real name in their username or on their profile. For Twitter, each Tweet obtained from the API comes with the username associated with the tweet. While this is often just a made up name, it's still enough to identify which Tweets were made by which users. However, since any user can easily make _many_ Twitter accounts it can be hard to know who's actually who, and helpful to think of tweeters as semi-anonymous.\n",
    "+ De-anonymized - This kind of data can be rare, but most often is hard to get! This is data with names etc. and other information registered and controlled closely against account-official identifiers. It's hard to find many examples outside of in-house corporate datasets, but in a sense this is exactly what Facebook strives for. Have you ever had Facebook demand a picture of your photo id for account verification? It happens!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Shared Tasks\n",
    "Why else share data? Well, sometimes it's important for a community to come together around solving a problem on common data. It allows folks to compare results exactly and identify best solutions. Such organized events are referred to as _shared tasks_. \n",
    "\n",
    "Shared tasks are somewhat competitive endeavors in which a group of participants are all given the same dataset with a specific task in mind, which should have a precise mechanism for evaluation. The task lasts for a specific amount of time, after which the participants are evaluated on how well they achieved the task, and the winners are selected. Prizes vary depending on the context of the shared task, but sometimes they can be lucrative. Apart from the prizes, winners of shared tasks can claim (rightfully so) mastery of that specific subfield of machine learning or data analysis. So, people take shared tasks quite seriously, and they are often very popular. \n",
    "\n",
    "Having a shared task centered around a dataset provides a lot of top-notch exposure! One avenue for shared tasks is the academic conference systems. This is great for academics, but what about others who are still interested in data science and showing off their skills? Probably the best place for that is Kaggle.\n",
    "\n",
    "### 9.3.1 Kaggle\n",
    "If you haven't yet, check out Kaggle: \n",
    "\n",
    "- https://www.kaggle.com/\n",
    "\n",
    "It's a platform for practicing and learning about data science. Notably, Kaggle hosts a huge amount of public datasets and encourages anyone and everyone to download them and play around with them. Generally, the rights for these have been secured by the dataset posters, which range from businesses (generating data) to excited ML researchers interested in exposure and engagement. When companies post their datasets/shared tasks there are often excellent prizes and opportunities for recognition available to the winners of these contests! Also, many companies pay close attention the results. \n",
    "\n",
    "#### 9.3.1.1 Exercise: Kaggle \n",
    "Set up a Kaggle account (if you don't have one) and do a bit of browsing. Another feature that's handy on Kaggle is that it allows for users to upload their own datasets (both privately or publicly). Thus, even if you're more interested in the data acquisition and consolidation aspects of data science, Kaggle is still quite a useful potential tool. If you upload a public dataset and it proves popular, just as much recognition awaits.\n",
    "\n",
    "### 9.3.2 Extended example: A Twitter geolocation shared task and dataset\n",
    "Should we expect researchers focused on a ML shared task to write their own data access software for a shared task, even if we provide the identifiers necessary to build the requesite data? Well, even if the community of task takers includes some folks with data acquisition savvy you're probably finding by now that this work is very non-trivial. So, in addition to pointers to content an essential ingredient for data distribution is the provisioning of reusable data access software.\n",
    "\n",
    "#### 9.3.2.1 High level considerations\n",
    "When a script which downloads a specific set of Tweets is distributed, the Tweets themselves are not being sent out. All that is being sent is a program which will automatically download the Tweets using Twitter's REST API (provided that you have signed up for API access and are willing to submit your API keys). This may seem like a silly trick to get around copyright laws, but instead of sending out copyrighted data these scripts require users to go through the official APIs created for use with social media platforms. This is the only way that most social media platforms would like for users to obtain large quantities of data. \n",
    "\n",
    "#### 9.3.2.2 An exemplar shared task\n",
    "Let's look at an example of exactly this distribution model. At the 2016 W-NUT (Workshop on Noisy User Generated Text) conference, there was a shared task where the competitors were given a large corpus of Tweets along with a list of potential urban centers, and were asked to determine which urban center each Tweet originated from. The task is detailed here: \n",
    "\n",
    "- https://noisy-text.github.io/2016/geo-shared-task.html. \n",
    "\n",
    "#### 9.3.2.3 Getting the data\n",
    "What's important to us right now is how the training data was distributed. Upon inspection, you'll find the following link to a Google Drive which contains the training data downloader script: \n",
    "\n",
    "- https://drive.google.com/drive/folders/0B8bfAiuVjZ1Edk5hYkhnbEF3SDg. \n",
    "\n",
    "The files we'll focus on here are `README`,`validation.tweet.json.gz`, `cred.txt`, and `tweet_downloader.py`.\n",
    "\n",
    "#### 9.3.2.4. README: understanding the shared task materials\n",
    "Since the file format chose was a Google doc, i.e., Microsoft Word (tsk), its contents are converted to markdown (yay) in the below cell. Let's review some highlights:\n",
    "1. From the first line: data files won't contain \"raw\", i.e., text content or metadata besides the tweet ids.\n",
    "- Samples are provided for how the shared task data should look, once downloaded.\n",
    "- Reviewing the `train.user.json` sample, the target data (once tweets are downloaded) joins the tweets to the known city and tweet latitude and longitude values. __Important__: over 12 million tweets are covered by this file; this should make us second guess passing this data around on github; checking on the Google Drive folder shows that this file is over 600 MB compressed! That's over 6 times the file size limit on github, and it'll be a much bigger decompressed!\n",
    "- To handle API access control, users are required to place tokens (credentials) in the `cred.txt` file. As we'll see when reviewing tweet_downloader.py how this works exactly.\n",
    "- The \"test\" data files contain \"hashed\" user and tweet ids. These allow individuals to work with the data according to the metadata available from Twitter without letting the user know how to get the original data from Twitter. __Extremely important__: This means the test data are distributed directly; the only reason this is allowed is because Twitter's policy allows sharing datasets informally (directly) so long as the contain fewer than 50k tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Due to Twitter term restrictions, we are not going to share the raw data but the tweet ids for this research purpose shared task.\n",
    "> - The downloader script is to download tweet JSON data using the tweet ids in the data folder. \n",
    "> - You are also required to register a Twitter dev account to get your credentials and put them as specified in cred.txt. (https://apps.twitter.com/)\n",
    "> - The folder also contains train/dev/test data for WNUT 2016 Geotagging Shared Task\n",
    ">\n",
    "> File: train.user.json <br>\n",
    "> Description: Training data for user level tasks <br>\n",
    "> Size: 1M users, 12,827,165 tweets <br>\n",
    "> Format: Each line is a JSON dict which consists of tweet_id, user_id, city_name, city_latitude, city_longitude, tweet_latitude, tweet_longitude, tweet_text <br>\n",
    "> Example:  <br>\n",
    "> \n",
    "> ```\n",
    "> {\n",
    ">   \"tweet_id\": 12345678,\n",
    ">   \"user_id\": 12345,\n",
    ">   \"city_name\": \"melbourne-07-au\",\n",
    ">   \"city_latitude\": -35.123,\n",
    ">   \"city_longitude\": 108.123,\n",
    ">   \"tweet_latitude\": -35.124,\n",
    ">   \"tweet_longitude\": 108.124,\n",
    ">   \"tweet_text\": \"This is a demo text\"\n",
    "> }\n",
    "> ```\n",
    "> <br>\n",
    "> File: validation.user.json <br>\n",
    "> Description: validation data for user level tasks <br>\n",
    "> Size: 10K users, 128,524 tweets <br>\n",
    "> Format: same as train.user.json <br>\n",
    "> Example: same as train.user.json <br>\n",
    ">  <br>\n",
    "> File: test.user.json <br>\n",
    "> Description: Test data for user level tasks <br>\n",
    "> Size: 10K users, 99,732 tweets <br>\n",
    "> Format: Each line is a JSON dict which consists of hashed_tweet_id, hashed_user_id, tweet_text <br>\n",
    "> Example: <br>\n",
    "> \n",
    "> ```\n",
    "> {\n",
    ">   \"hashed_tweet_id\": 25D55AD283AA400AF464C76D713C07AD,\n",
    ">   \"hashed_user_id\": 827CCB0EEA8A706C4C34A16891F84E7B,\n",
    ">   \"tweet_text\": \"This is a demo text\"\n",
    "> }\n",
    "> ```\n",
    "> <br>\n",
    "> File: train.tweet.json <br>\n",
    "> Description: Training data for tweet level tasks <br>\n",
    "> Size: same as train.user.json <br>\n",
    "> Format: same as train.user.json <br>\n",
    "> Example: same as train.user.json <br>\n",
    "> <br>\n",
    "> File: validation.tweet.json <br>\n",
    "> Description: validation data for tweet level tasks <br>\n",
    "> Size: 128,435 tweets which are different from validation.user.json <br>\n",
    "> Format: same as validation.user.json except the user related fields are omitted. <br>\n",
    "> Example: same as validation.user.json except the user related fields are omitted. <br>\n",
    "> <br>\n",
    "> File: test.tweet.json <br>\n",
    "> Description: Test data for tweet level tasks <br>\n",
    "> Size: 10K tweets which are different from test.user.json <br>\n",
    "> Format: Each line is a JSON dict which consists of hashed_tweet_id, hashed_user_id, tweet_text <br>\n",
    "> Example: <br>\n",
    "> \n",
    "> ```\n",
    "> {\n",
    ">  \"hashed_tweet_id\": 25D55AD283AA400AF464C76D713C07AD,\n",
    ">  \"hashed_user_id\": 827CCB0EEA8A706C4C34A16891F84E7B,\n",
    ">  \"tweet_text\": \"This is a demo text\"\n",
    "> }\n",
    "> ```\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.2.5 Understanding the data\n",
    "While `README` provides some high level understanding, there's nothing like some exploration using Python to really dig in what we've got. Note: to actually use the data we first have to decompress (unzip). This can be done using `gunzip filename.gz` from the command line, like:\n",
    "- `gunzip validation.tweet.json.gz`\n",
    "\n",
    "Note: each of the supplied datasets are actually in line-by-line json format. This allow the data to be downloaded incrementally (or from separate Twitter apps/accounts), without having to load the entire dataset in memory. Since the validation file is small enough we'll just load it all into memory using a loop. \n",
    "\n",
    "Discussion: The task developers (in the `tweet_downloader.py` script) rely on the `tweet_id` field in their task data to access the referenced tweets. Once downloaded, we'll have to join the tweets with the task data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tweet_id': '360011809459146752',\n",
       "  'tweet_city': 'mansfield-engj9-gb',\n",
       "  'tweet_latitude': '53.130483',\n",
       "  'tweet_longitude': '-1.141419'},\n",
       " {'tweet_id': '547240490022617088',\n",
       "  'tweet_city': 'omaha-ne055-us',\n",
       "  'tweet_latitude': '41.782947',\n",
       "  'tweet_longitude': '-95.287009'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "validation = []\n",
    "with open(\"data/validation.tweet.json\") as f:\n",
    "    for line in f:\n",
    "        validation.append(json.loads(line))\n",
    "validation[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.2.6 Understanding the script\n",
    "To review, the code is placed in a cell below under the `%%writefile tweet_downloader.py` IPython magic command. For the uninitiated, this magic command simply writes the contents of the cell to the specified file (assuming a relative path to the notebook).\n",
    "\n",
    "Note: it may seem a bit confusing (because of the parsing they do), but it's really just taking a list of Tweets and downloading them using the REST API, much like in __Chapter 3__. We'll take some aspects from their approach below and simplify their code into a more Python user-freindly version written as a module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tweet_downloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tweet_downloader.py\n",
    "#!//usr/bin/env python\n",
    "\"\"\"\n",
    "Description: \n",
    "    Download tweets using tweet ID, downloaded from https://noisy-text.github.io/files/tweet_downloader.py\n",
    "\n",
    "Usage example (in linux):\n",
    "    clear;python tweet_downloader.py --credentials ../data/credentials.txt --inputfile ../data/input.tids --outputtype IdTweetTok\n",
    "\n",
    "Inputfile contains training/validation data whose first column is tweetID\n",
    "\n",
    "credentials.txt stores the Twitter API keys and secrects in the following order:\n",
    "consumer_key\n",
    "consumer_secret\n",
    "access_token\n",
    "access_token_secret\n",
    "\n",
    "Required Python library: \n",
    "    ujson, twython and twokenize (https://github.com/myleott/ark-twokenize-py)\n",
    "\n",
    "An example output with whitespace tokenised text and tweet id in JSON format\n",
    "    {\"text\":\"@SupGirl whoaaaaa .... childhood flashback .\",\"id_str\":\"470363741880463362\"}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    import ujson as json\n",
    "except ImportError:\n",
    "    import json\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "from twython import Twython, TwythonError\n",
    "from collections import OrderedDict\n",
    "\n",
    "MAX_LOOKUP_NUMBER = 100\n",
    "SLEEP_TIME = 15 + 1\n",
    "twitter = None\n",
    "arguments = None\n",
    "tid_list = None\n",
    "\n",
    "def init():\n",
    "    global twitter, arguments, tid_list\n",
    "\n",
    "    parser = argparse.ArgumentParser(description = \"A simple tweet downloader for WNUT-NORM shared task.\")\n",
    "    parser.add_argument('--credentials', type=str, required = True, help = '''\\\n",
    "        Credential file which consists of four lines in the following order:\n",
    "        consumer_key\n",
    "        consumer_secret\n",
    "        access_token\n",
    "        access_token_secret\n",
    "        ''')\n",
    "    parser.add_argument('--inputfile', type=str, required = True, help = 'Input file one tweet id per line')\n",
    "    parser.add_argument('--outputtype', type=str, default='IdTweet', choices = ['json', 'IdTweet'], help = '''\\\n",
    "        Output data type:\n",
    "        (1) json: raw JSON data from Twitter API;\n",
    "        (2) IdTweet: tweet ID and raw tweet messages (default)\n",
    "        ''')\n",
    "    arguments = parser.parse_args()\n",
    "\n",
    "    credentials = []\n",
    "    with open(arguments.credentials) as fr:\n",
    "        for l in fr:\n",
    "            credentials.append(l.strip())\n",
    "    twitter = Twython(credentials[0], credentials[1], credentials[2], credentials[3])\n",
    "\n",
    "    tid_list = []\n",
    "    with open(arguments.inputfile) as fr:\n",
    "        for l in fr:\n",
    "            jobj = json.loads(l.strip())\n",
    "            tid = jobj['tweet_id']\n",
    "            tid_list.append(tid)\n",
    "\n",
    "def download():\n",
    "    global twitter, arguments, tid_list\n",
    "    with open(arguments.inputfile + \".\" + arguments.outputtype, \"w\") as fw:\n",
    "        tid_number = len(tid_list)\n",
    "        max_round = tid_number // MAX_LOOKUP_NUMBER + 1\n",
    "        for i in range(max_round):\n",
    "            tids = tid_list[i * MAX_LOOKUP_NUMBER : (i + 1) * MAX_LOOKUP_NUMBER]\n",
    "            time.sleep(SLEEP_TIME)\n",
    "            jobjs = []\n",
    "            jobjs = twitter.lookup_status(id = tids)\n",
    "            for jobj in jobjs:\n",
    "                if arguments.outputtype == \"json\":\n",
    "                    fw.write(json.dumps(jobj))\n",
    "                else:\n",
    "                    tweet = jobj[\"text\"]\n",
    "                    tid = jobj[\"id_str\"]\n",
    "                    dic_tweet = (('tweet_id', tid), ('text', tweet))\n",
    "                    fw.write(json.dumps(OrderedDict(dic_tweet)))\n",
    "                fw.write(\"\\n\")\n",
    "\n",
    "def main():\n",
    "    init()\n",
    "    download()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3.2.7 Rewriting the script\n",
    "So, it's finally come back to haunt us. The `tweet_downloader.py` script was built using Python 2. To explore, let's spruce up the code and convert it to a Python _class_, i.e., as a self contained object. With this we'll be able to interact with the downloader in our notebook!\n",
    "\n",
    "Cutting away from the original tweet downloader's special modules (`ujson` for faster json handling, `OrderedDict` to order of input tweet numbers for matched serialization, and `argparse` utilities for command line argument parsing) we can see that there's only a few things we need for Pythonic interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, time\n",
    "from twython import Twython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice the scattering of `global` variables declarations above. Rather than use these, Python's classes have the `self` argument that provides the managemnt of class attributes, i.e., objects that the class should have access to across its methods.\n",
    "\n",
    "Architecturally, the shared task's downloader aligns to two methods, which we'll call:\n",
    "- `__init__`: The shared task's `init()` is ripe conversion into Python's special class initialization method. In brief, the class's defined `__init()__` method can be expected to run at the time the class is instantiated into an object. Importantly, defining this method provides the opportunity to specify how arguments are input to the class.\n",
    "- `download`: Converting the main action of the shared task code, we can match a with a `.download()` method that 'activates' the downloader to access batches of tweets. Note that this method implements our dance with Twitter's [rate limit](https://developer.twitter.com/en/docs/basics/rate-limiting.html), which should push the code to target roughly 1 request per minute (no more than 15 per 15-minute window will be allowed).\n",
    "\n",
    "We've also made a few improvements to the code's function:\n",
    "- The original slept for fixed time, while the new offsets against the time time the downloading process takes between batches (API calls).\n",
    "- The downloaded tweets for all specified in a target file. Since it's possible to run the download process in parallel if the user has multiple apps/credentials, we can modify as below to have the downloader only request a specified subset (arguments `first` and `last` indices) from Twitter, i.e., target the downloader to a subset of the dataset. Note that since the original stored the data using Python's wrte mode (`'w'`) that we've had to modify to append mode (`'a'`) to enable multiple downloaders.\n",
    "\n",
    "Note: Unlike `Twython`'s `.show_status(id)` method the downloader has us using `.lookup_status(id_list)`, which has the benefit of accepting a list of tweet ids to download; the maximum value is 100 tweets as per Twitter's limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_downloader(object):\n",
    "\n",
    "    ## initialize the tweet downloader\n",
    "    def __init__(self, inputfile, credfile = \"cred.txt\", first = 0, last = float('Inf'),\n",
    "                 MAX_LOOKUP_NUMBER = 100, SLEEP_TIME = 60):\n",
    "        \n",
    "        ## accept user input\n",
    "        self.MAX_LOOKUP_NUMBER = MAX_LOOKUP_NUMBER\n",
    "        self.SLEEP_TIME = SLEEP_TIME\n",
    "        self.inputfile = inputfile\n",
    "        self.credfile = credfile\n",
    "        self.first = first\n",
    "        self.last = last\n",
    "\n",
    "        ## load the specified Twitter app credentials\n",
    "        self.credentials = []\n",
    "        with open(\"cred\"+os.path.sep+self.credfile) as f:\n",
    "            for line in f:\n",
    "                self.credentials.append(line.strip())\n",
    "        \n",
    "        ## initialize the api client\n",
    "        self.twitter = Twython(self.credentials[0], self.credentials[1], \n",
    "                               self.credentials[2], self.credentials[3])\n",
    "        \n",
    "        ## load a list of tweet IDs to download\n",
    "        self.tid_list = []\n",
    "        with open(self.inputfile) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                ## make sure this tweet is in specified range on list\n",
    "                if i < self.first or i > self.last: continue\n",
    "                data = json.loads(line.strip())\n",
    "                self.tid_list.append(data['tweet_id'])\n",
    "\n",
    "    ## download the tweets on the list \n",
    "    def download(self):\n",
    "        ## the total number of tweets to download\n",
    "        tid_number = len(self.tid_list)\n",
    "        ## compute the number of batches to download\n",
    "        max_round = tid_number // self.MAX_LOOKUP_NUMBER + 1\n",
    "        ## initialize time counters\n",
    "        now, then = 0, 0\n",
    "        ## loop over batches\n",
    "        for i in range(max_round):\n",
    "            ## slice out the tweet ids for this batch\n",
    "            tids = self.tid_list[i * self.MAX_LOOKUP_NUMBER : \n",
    "                                 (i + 1) * self.MAX_LOOKUP_NUMBER]            \n",
    "            ## advance time counters\n",
    "            then = now; now = time.time()\n",
    "            ## compute remaining time\n",
    "            REMAINING_SLEEP = self.SLEEP_TIME - int(now - then) + 1\n",
    "            ## only sleep if we have already made previous calls\n",
    "            if then:\n",
    "                time.sleep(REMAINING_SLEEP)\n",
    "            ## download the tweets\n",
    "            data = self.twitter.lookup_status(id = tids)\n",
    "            then = time.time()\n",
    "            ## open the output file in append mode (in case of multiple downloaders)\n",
    "            with open(re.sub(\".json\",\".downloaded.json\",self.inputfile), \"a\") as f:\n",
    "                ## store results\n",
    "                for d in data:\n",
    "                    f.write(json.dumps(d)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's straightforward for our code to access the first 10 tweets in the `validation.tweet.json` file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize downloader\n",
    "downloader = tweet_downloader(inputfile = \"data/validation.tweet.json\",  \n",
    "                              last = 10)\n",
    "## run downloader\n",
    "downloader.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've downloaded (some of) the data, we may want to use it. Is it ready? Well since the geospatial data for the shared task is still in `validation.tweet.json`, and the newly downloaded full-content tweets are in a separate file, `validation.tweet.downloaded.json`, we'll have to join the two portions of data by tweet id. Building off of the loading code from __Section 9.3.2.5__, let's build a function that matches integerates the shared task's geospatial data into any of the tweets we've managed to of download using our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_tweets(inputfile):\n",
    "    ## initialize the integrated data and open downloaded tweets\n",
    "    tweets = {}\n",
    "    with open(re.sub(\".json\", \".downloaded.json\", inputfile)) as f:\n",
    "        for line in f: ## each line is a tweet\n",
    "            tweet = json.loads(line)\n",
    "            ## if we store the tweet id as its key we can use infullness below\n",
    "            tweets[tweet[\"id_str\"]] = tweet\n",
    "            ## set a default value for missing task data\n",
    "            tweets[tweet[\"id_str\"]][\"task_data\"] = None\n",
    "    ## open the original (geospatial data) shared task file\n",
    "    with open(inputfile) as f:\n",
    "        for line in f: ## each line corresponds to a tweet\n",
    "            task_data = json.loads(line)\n",
    "            ## check if this task data's id matches the downloaded\n",
    "            if task_data[\"tweet_id\"] in tweets:\n",
    "                ## store the task data over the default\n",
    "                tweets[task_data[\"tweet_id\"]][\"task_data\"] = task_data\n",
    "    return(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Tue Dec 31 16:45:56 +0000 2013',\n",
       " 'id': 418060465164406785,\n",
       " 'id_str': '418060465164406785',\n",
       " 'text': 'Happy new year everybody :)))) http://t.co/jN7eokdWiX',\n",
       " 'truncated': False,\n",
       " 'entities': {'hashtags': [],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'urls': [],\n",
       "  'media': [{'id': 418060465055354880,\n",
       "    'id_str': '418060465055354880',\n",
       "    'indices': [31, 53],\n",
       "    'media_url': 'http://pbs.twimg.com/media/Bc0_xypCUAAwpKC.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/Bc0_xypCUAAwpKC.jpg',\n",
       "    'url': 'http://t.co/jN7eokdWiX',\n",
       "    'display_url': 'pic.twitter.com/jN7eokdWiX',\n",
       "    'expanded_url': 'https://twitter.com/intanpitha/status/418060465164406785/photo/1',\n",
       "    'type': 'photo',\n",
       "    'sizes': {'large': {'w': 333, 'h': 333, 'resize': 'fit'},\n",
       "     'medium': {'w': 333, 'h': 333, 'resize': 'fit'},\n",
       "     'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'small': {'w': 333, 'h': 333, 'resize': 'fit'}}}]},\n",
       " 'extended_entities': {'media': [{'id': 418060465055354880,\n",
       "    'id_str': '418060465055354880',\n",
       "    'indices': [31, 53],\n",
       "    'media_url': 'http://pbs.twimg.com/media/Bc0_xypCUAAwpKC.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/Bc0_xypCUAAwpKC.jpg',\n",
       "    'url': 'http://t.co/jN7eokdWiX',\n",
       "    'display_url': 'pic.twitter.com/jN7eokdWiX',\n",
       "    'expanded_url': 'https://twitter.com/intanpitha/status/418060465164406785/photo/1',\n",
       "    'type': 'photo',\n",
       "    'sizes': {'large': {'w': 333, 'h': 333, 'resize': 'fit'},\n",
       "     'medium': {'w': 333, 'h': 333, 'resize': 'fit'},\n",
       "     'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'small': {'w': 333, 'h': 333, 'resize': 'fit'}}}]},\n",
       " 'source': '<a href=\"http://blackberry.com/twitter\" rel=\"nofollow\">Twitter for BlackBerry</a>',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'user': {'id': 79375476,\n",
       "  'id_str': '79375476',\n",
       "  'name': 'in',\n",
       "  'screen_name': 'indwinov',\n",
       "  'location': 'Seoul, Republic of Korea',\n",
       "  'description': 'I love Allah (berpijak diatas kaki sendiri)',\n",
       "  'url': None,\n",
       "  'entities': {'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 538,\n",
       "  'friends_count': 350,\n",
       "  'listed_count': 4,\n",
       "  'created_at': 'Sat Oct 03 04:37:36 +0000 2009',\n",
       "  'favourites_count': 156,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': True,\n",
       "  'verified': False,\n",
       "  'statuses_count': 12552,\n",
       "  'lang': 'en',\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': '888D94',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/1023755366486568961/uKFMQoTN_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1023755366486568961/uKFMQoTN_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/79375476/1532917306',\n",
       "  'profile_link_color': '97AFB8',\n",
       "  'profile_sidebar_border_color': '000000',\n",
       "  'profile_sidebar_fill_color': 'F5DCDD',\n",
       "  'profile_text_color': 'D19797',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': False,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'none'},\n",
       " 'geo': {'type': 'Point', 'coordinates': [-6.34569361, 108.33657722]},\n",
       " 'coordinates': {'type': 'Point', 'coordinates': [108.33657722, -6.34569361]},\n",
       " 'place': {'id': '6d195c5070bf72b4',\n",
       "  'url': 'https://api.twitter.com/1.1/geo/id/6d195c5070bf72b4.json',\n",
       "  'place_type': 'city',\n",
       "  'name': 'Indramayu',\n",
       "  'full_name': 'Indramayu, Indonesia',\n",
       "  'country_code': 'ID',\n",
       "  'country': 'Indonesia',\n",
       "  'contained_within': [],\n",
       "  'bounding_box': {'type': 'Polygon',\n",
       "   'coordinates': [[[108.277302, -6.404803],\n",
       "     [108.3766145, -6.404803],\n",
       "     [108.3766145, -6.23671],\n",
       "     [108.277302, -6.23671]]]},\n",
       "  'attributes': {}},\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 0,\n",
       " 'favorite_count': 0,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'possibly_sensitive': False,\n",
       " 'lang': 'en',\n",
       " 'task_data': {'tweet_id': '418060465164406785',\n",
       "  'tweet_city': 'masjid jamie baitul muttaqien-30-id',\n",
       "  'tweet_latitude': '-6.34569361',\n",
       "  'tweet_longitude': '108.33657722'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = integrate_tweets(inputfile = \"data/validation.tweet.json\")\n",
    "list(tweets.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
