{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Problem A (25 points)\n",
    "\n",
    "__A1.__ In this problem, you will be working with the [Seinfeld Chronicles dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles). Create an account on [Kaggle](https://www.kaggle.com) and download the `scripts.csv` file from the dataset and move it into the `data` directory. Read the `data/scripts.csv` file as a text file line-by-line and examine the list you have loaded the data into. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2.__ Is it possible to work with this data, simply splitting by a delimiter? Explain any complexity in the data's structured format that necessitates an established format-specific file reader. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3.__ Use the `csv` module to read the contents of the `data/scripts.csv` file into a list. Examine this list. How many unique speaking characters are present in the scripts in total? (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A4.__ Count the dialogue entries for the four major characters, \"JERRY\", \"GEORGE\", \"ELAINE\", and \"KRAMER\", using a dictionary (you are not allowed to use the Counter data structure for any component of this problem). (2 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A5.__ Count the number of words spoken by each of the main characters using a dictionary. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6.__ Count how many times each word is spoken by the main characters using a dictionary, then sort these word counts in descending order, i.e. from the most commonly spoken words to least. [__Hint__: You can use either a lambda function or a list comprehension to do this.] (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A7.__ Load the `data/stop-words.txt` file into a list. Find the 10 most common words for each of the main characters that are not in this list of stop words. Put these most common words in a dictionary data strucutre. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem B (25 points)\n",
    "\n",
    "__B1.__ You will be using part of the [Goodbooks 10k dataset](https://github.com/zygmuntz/goodbooks-10k) for this problem. Read the `data/goodreads-books.csv` file into a list. Create a dictionary for each book in the list that contains these fields: authors, original title, original publication year, average rating, and ratings count. (You should convert average rating and ratings count into `float` and `int` types.) Put all these metadata dictionaries into a list. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B2.__ Write a function to sort this list of book metadata in descending order of average rating. The function should take the list of metadata dictionaries as an input argument. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B3.__ Update the function to take two arguments: the list, and an integer value for minimum ratings count. The function should now sort _and_ filter the list, returning a list of books sorted by average rating that have been rated by more than a specified number of users. You can use three different approaches: loops, comprehensions, and the built-in `filter()` function (look up documentation and examples). (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus:__ Use all three approaches to write three different versions of the function. See if you can condense your code into a single line for some of the approaches! (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B3.__ Why is using a function for this task prudent? What do you think is an acceptable minimum ratings count? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem C (50 points)\n",
    "\n",
    "This problem deals with finding \"pangrams\" in text. A pangram is a sentence containing all 26 letters of the alphabet. `x` and `y` in the cell below are example sentences, `x` is a pangram, `y` is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Jim quickly realized that the beautiful gowns are expensive.\"\n",
    "y = \"This sentence is most certainly not a pangram.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C1.__ Define a generator function, `indices()`, that takes a string as input and outputs the index numbers where a letter occurs for the first time in the string. [__Hint:__ you can compare letters like numbers. For example, `char >= \"a\"` is a valid conditional statement. You can use this to check whether characters in a string are letters.] (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C2.__ Define a function, `verify()`, that takes a string as input and uses the `indices()` function to check if the string is a pangram. The output should be boolean `True` or `False`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C3:__ Write a version of `verify()` named `tiny_verify()` that performs the check in a single line of code, without using `indices()`. [__Hint:__ Use a comprehension.] (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C4.__ Modify the `verify()` function to figure out which letters (if any) are missing from a purported pangram. This version should return the list of missing letters instead of a boolean value. [__Hint:__ You can get a string containing all the letters of the alphabet by importing `ascii_lowercase` from the `string` module.] (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C5.__ Load and iterated through the collected [list of pangrams](http://clagnut.com/blog/2380/) in `data/pangrams.txt` line by line and determine if they are actually pangrams. Print out any lines that are not actually pangrams, and also the letters that are missing. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C6:__ Use the output from the `verify()` function to fix the failed pangrams, and verify that you have fixed them. (5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C7.__ In the cell below are provided some information about a set of books. Create a data object that holds the book numbers and titles associated to each authors's name. Write this out as a JSON file in the `data/books/` directory using the following schema. (5 points)\n",
    "\n",
    "`\n",
    "books = {\n",
    "    AuthorName: {\n",
    "        BookNumber: BookTitle,\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 84.txt; Frankenstein, or the Modern Prometheus; Mary Wollstonecraft (Godwin) Shelley\n",
    "# 98.txt; A Tale of Two Cities; Charles Dickens \n",
    "# 161.txt; Sense and Sensibility; Jane Austen\n",
    "# 730.txt; Oliver Twist or the Parish Boy's Progress; Charles Dickens\n",
    "# 768.txt; Wuthering Heights; Emily BrontÃ«\n",
    "# 1322.txt; Leaves of Grass; Walt Whitman\n",
    "# 1342.txt; Pride and Prejudice; Jane Austen\n",
    "# 1400.txt; Great Expectations; Charles Dickens\n",
    "# 2701.txt; Moby Dick; or the Whale; Herman Melville\n",
    "# 4300.txt; Ulysses; James Joyce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C8.__ Write a function, `get_pangrams()`, that takes a book number and outputs a list of the book's pangram sentences and the total number of sentences in the book. You will need to use the `re` (regular expressions) module to split the book text into sentences using the `re.split(pattern, string)` function. The pattern you will need is `\"[\\.\\?\\!][^a-zA-Z]\"`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C9.__ Determine who is the pangrammiest author and what the pangrammiest book is, as determined by pangrams per sentence. [__Hint:__ Use `defaultdict`s to create \"pangrams by author\" and \"pangrams by book\" objects.] (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus:__ Print out the most efficient pangram and its author and book, as determined by fewest characters. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
